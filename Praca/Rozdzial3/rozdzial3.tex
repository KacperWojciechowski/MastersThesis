\chapter{Wykorzystane zestawy danych}
\section{Omówienie danych eksperymentalnych}

W celu zestawienia funkcjonalnego bibliotek uczenia maszynowego w języku C++ i przedstawienia przykładów konieczne było wybranie danych eksperymentalnych możliwych do wykorzystania jako porównawczy punkt odniesienia. W tym celu, dla pełnego przetestowania wybranych funkcjonalności przygotowano zestaw danych do problemu klasyfikacji binarnej oraz zadania regresji.

\subsection{Dane klasyfikacyjne}
	
	 Jako dane klasyfikacyjne wybrano bazę dotyczącą diagnostyki raka piersi ,,\textit{Wisconsin Diagnostic Breast Cancer}'' z listopada 1995 roku, w której zamieszczono wyniki obrazowania określone w sposób liczbowy. Autorami zestawu są Dr. Wiliam H. Wolberg, W. Nick Street oraz Olvi L. Mangasarian z Uniwersytetu Wisconsin \cite{wisconsin}. Baza ta jest dostępna do pobrania z repozytorium Uniwersytetu Californii \cite{Dua:2019}. Dane mają następującą strukturę:
	
	\begin{enumerate}
		\item [1)] ID - numer identyfikacyjny pacjentki;
		\item [2)] Diagnosis [\textit{Malignant - M} / \textit{Benign - B}] - charakter nowotworu (złośliwy / łagodny), \textbf{zmienna odpowiedzi};
		\item [3)] Dane klasyfikujące:
			\begin{enumerate}
				\item [a)] \textit{Radius} - średnica guza;
				\item [b)] \textit{Texture} - tekstura guza;
				\item [c)] \textit{Perimeter} - obwód guza;
				\item [d)] \textit{Area} - pole guza;
				\item [e)] \textit{Smoothness} - gładkość, miara lokalnych różnic w promieniu guza;
				\item [f)] \textit{Compactness} - zwartość, wykorzystywana do oceny stadium guza;
				\item [g)] \textit{Concavity} - stopień wklęsłości miejsc guza;
				\item [h)] \textit{Concave points} - punkty wklęsłości guza;
				\item [i)] \textit{Symmetry} - symetria guza, pomagająca w ocenie charakteru przyrostu guza;
				\item [j)] \textit{Fractal dimention (,,coastline approximation'' - 1)} - wymiar fraktalny pozwalający na ilościowy opis złożoności komórek nerwowych, umożliwiający stwierdzenie nowotworzenia się zbioru komórek.
			\end{enumerate}
	\end{enumerate}
	
	Dla każdej ze zmiennych odpowiedzi określono wartość średnią, odchylenie standardowe oraz średnią trzech największych pomiarów, gdzie każdy zestaw ustawiony jest sekwencyjnie (np. kolumna 3 - średni promień, kolumna 12 - odchylenie standardowe promienia, kolumna 22 - średnia trzech największych pomiarów promienia). Każda ze zmiennych ma charakter ciągły. Zredukowany zestaw danych, zawierający jedynie zmienne decyzyjne informujące o średnich wartościach znaleźć można jako dodatek do podręcznika \cite{biostatisticsJMP}.
	
\subsection{Dane regresyjne}

	Do demonstracji problemu regresji wykorzystano zestaw danych ,,IronGlutathione'' dołączony do podręcznika \cite{biostatisticsJMP}, dotyczące badań nad związkiem między zawartością żelaza, a enzymem transferazy glutationowej w organiźmie człowieka. Obserwacje pochodzą z badań z 2012 roku. Zestaw obejmuje 90 obserwacji i dotyczy 10 zmiennych:
	
	\begin{enumerate}
		\item \textit{Age} - wiek badanej osoby;
		\item \textit{Gender} - płeć osoby;
		\item \textit{Alpha GST (ng/L)} - zawartość transferazy glutationinowej typu $\alpha$;
		\item \textit{pi GST (mg/L)} - zawartość transferazy glutationinowej typu $\pi$;
		\item \textit{transferrin (mg/mL)} - zawartość transferyny;
		\item \textit{sTfR (mg/mL)} - zawartość rozpuszczalnego receptora transferyny;
		\item \textit{Iron (mg/dL)} - zawartość żelaza;
		\item \textit{TIBC (mg/dL)} - całkowita zdolność wiązania żelaza;
		\item \textit{\%ISAF (Iron / TIBC)} - współczynnik nasycenia transferyny;
		\item \textit{Ferritin (ng/dL)} - zawartość ferrytyny;
	\end{enumerate} 
	
	Z racji na większą swobode w wyborze zmiennej odpowiedzi w przypadku danych regresyjnych, zdecydowano się na wybór ostatniej zmiennej (\textit{Ferritin (ng/dL)}) jako odpowiedzi. 
	
\section{Specyfika danych}

	Podczas przeprowadzenia procesu uczenia maszynowego, jednym z najistotniejszych kroków jakie należy podjąć jest wstępne zaznajomienie się z zestawem danych i jego analiza pod kątem rozkładu poszczególnych zmiennych. W tym celu wykorzystane zostało oprogramowanie JMP Pro \cite{jmp}. 

	\subsection{Dane klasyfikacyjne}
	\subsubsection{Analiza rozkładów predyktorów i odpowiedzi}
		
	Proces analizy rozkładu rozpoczęto od scharakteryzowania zmiennej odpowiedzi (\textit{Diagnosis}). Rysunek \ref{fig:diagnosisdistribution} przedstawia uzyskany histogram, wraz z tabelą określającą liczby obserwacji danej klasy i prawdopodobieństwo przynależności odpowiedzi do danej klasy. Zauważyć można, że dla użytego zestawu danych zarejestrowano 357 obserwacji łagodnego raka piersi, a jego prawdopodobieństwo wynosi $\approx$ 62,7\%, natomiast do klasy \textit{Malignant} należało 212 obserwacji, co odpowiada prawodpodobieństwu $\approx$ 37,3\%.
		
	Podczas analizy histogramów predyktorów stwierdzono, że znaczna liczba ma charakter prawostronnie skośny oraz występują dla nich obserwacje odstające, o czym informuje znajdujący się po prawej stronie histogramu wykres pudełkowy, co przedstawiono na rys. \ref{fig:variabledistribution}. Wyjątkiem okazała się zmienna \textit{Mean Largest Concave Points}, która mimo lekkiej skośności, okazała się nie posiadać obserwacji odstających. Na podstawie tych informacji stwierdzono, że aby przygotować dane w odpowiedni sposób do procesu uczenia, należy przeprowadzić ich czyszczenie oraz normalizację rozkładu.
	
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=0.6\linewidth]{Rysunki/Rozdzial2/diagnosis_distribution}
		\caption{Histogram rozkładu zmiennej odpowiedzi.}
		\label{fig:diagnosisdistribution}
	\end{figure}
	
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=0.6\linewidth]{Rysunki/Rozdzial2/variable_distribution}
		\caption{Przykłady histogramów zmiennych decyzyjnych.}
		\label{fig:variabledistribution}
	\end{figure}
	
	
	\subsubsection{Czyszczenie i normalizacja rozkładu danych}
	
	Na pełny zestaw danych składa się 569 obserwacji. Podczas wstępnej analizy stwierdzono istnienie 13 brakujących wartości dla predyktora \textit{Std err concave points}, które uzupełniono wartością średnią z całej kolumny. Głównym problemem okazały się obserwacje odstające oraz skośności rozkładu. Do analizy obserwacji odstających wykorzystano wykresy pudełkowe, gdzie oś Y reprezentowała zmienną odpowiedzi, natomiast oś X czyszczoną zmienną decyzyjną. Przykładowy wykres został przedstawiony na rysunku \ref{fig:boxgraph}. Ze względu na bardzo małą liczbę obserwacji zdecydowano się rozpocząć proces przystosowywania danych do uczenia od normalizacji ich rozkładu, aby zminimalizować lub wyeliminować konieczność usunięcia danych odstających. 
	
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=0.7\linewidth]{Rysunki/Rozdzial2/box_graph}
		\caption{Przykład analizy obserwacji odstających dla poszczególnych klas zmiennej odpowiedzi.}
		\label{fig:boxgraph}
	\end{figure}
	
	W pierwszym podejściu zdecydowano się na zastosowanie transformacji logarytmicznej dla wszystkich zmiennych decyzyjnych i porównanie charakterystyk uzyskanych rozkładów z oryginalnymi. Zmienna \textit{Mean largest concave points} okazała się posiadać rozkład bardzo zbliżony do rozkładu normalnego, w związku z czym wyłączono ją z dalszej analizy normalizacji. Przykładowe wyniki przedstawiono na rysunku \ref{fig:log}. Transformacja ta okazała się skutecznym rozwiązaniem jedynie dla następujących zmiennych:
	
	\begin{enumerate}
		\item \textit{Mean radius};
		\item \textit{Mean texture};
		\item \textit{Mean perimeter},
		\item \textit{Mean area};
		\item \textit{Mean smoothness};
		\item \textit{Mean symmetry};
		\item \textit{Std err texture};
		\item \textit{Std err smoothness};
		\item \textit{Std err compactness};
		\item \textit{Std err concave points};
		\item \textit{Mean largest texture};
		\item \textit{Mean largest smoothness};
		\item \textit{Mean largest compactness}.
	\end{enumerate}

	\begin{figure}[!ht]
		\centering
		\includegraphics[width=0.7\linewidth]{Rysunki/Rozdzial3/log}
		\caption{Porównanie rozkładu danych przed i po transformacji logarytmicznej.}
		\label{fig:log}
	\end{figure}

	W drugim kroku podjęto próbę wykorzystania transformacji pierwiastkiem sześciennym dla pozostałych zmiennych decyzyjnych, ze względu na jej skuteczność dla danych o rozkładzie prawoskośnym. Rysunek \ref{fig:cuberoot} przedstawia porównanie rozkładu zmiennej \textit{Mean concavity} przed i po transformacji pierwiastkiem sześciennym. Pomyślnie znormalizowano rozkłady następujących zmiennych:
	
	\begin{enumerate}
		\item \textit{Mean compactness};
		\item \textit{Mean concavity};
		\item \textit{Mean concave points};
		\item \textit{Std err concavity};
		\item \textit{Mean largest radius};
		\item \textit{Mean largest perimeter};
		\item \textit{Mean largest concavity};
		\item \textit{Mean largest symmetry}.
	\end{enumerate} 

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.7\linewidth]{Rysunki/Rozdzial3/cube_root}
	\caption{Porównanie rozkładów danych przed i po zastosowaniu transformacji pierwiastkiem sześciennym.}
	\label{fig:cuberoot}
\end{figure}

	Ostatecznym krokiem okazało się zastosowanie odwrotnej transformacji Arrheniusa, wywodzącej się od wzoru Arrheniusa opisującego energię aktywacji reakcji chemicznej, wyrażonego wzorem \ref{eq_arr} \cite{arr}: 
	
	\begin{equation}
		\ln k = \ln A - \frac{E_a}{RT}
		\label{eq_arr}
	\end{equation}

	gdzie: 
	\begin{itemize}
		\item $k$ - stała współczynnika reaktywności chemicznej;
		\item $A$ - współczynnik częstotliwości;
		\item $E_a$ - energia aktywacji reakcji;
		\item $R$ - stała gazu idealnego;
		\item $T$ - temperatura reakcji w Kelvinach;
	\end{itemize}
	
	Niestety, część z uzyskanych zmodyfikowanych zmiennych decyzyjnych zachowała częściowy skośny rozkład, jednak inne przetestowane transformacje, jak m.in. pierwiastek kwadratowy, potęga kwadratowa, $\log(x+1)$, logarytm dziesiętny, funkcja potęgowa, funkcja wykładnicza, przyniosły rezultaty porównywalne lub gorsze od uzyskanego w wyniku w/w odwrotnej transformacji Arrheniusa. Rysunek \ref{fig:arrhenius} przedstawia porównanie uzyskanych rozkładów. Predyktory poddane odwrotnej transformacji Arrheniusa to:
	
	\begin{enumerate}
		\item \textit{Mean fractal dimension};
		\item \textit{Std err radius};
		\item \textit{Std err perimeter};
		\item \textit{Std err area};
		\item \textit{Std err symmetry};
		\item \textit{Std err fractal dimension};
		\item \textit{Mean largest area};
		\item \textit{Mean largest fractal dimension}.
	\end{enumerate}
	
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=0.7\linewidth]{Rysunki/Rozdzial3/arrhenius}
		\caption{Porównanie uzyskanych rozkładów danych przed i po odwrotnej transformacji Arrheniusa.}
		\label{fig:arrhenius}
	\end{figure}
	
	Ze względu na bardzo małą liczbę obserwacji, zdecydowano się na zachowanie wszystkich obserwacji odstających, aby zapobiec utracie informacji i zmianie uzyskanych w procesie normalizacji rozkładów. W celu zachowania kompatybilności z bibliotekami omawianymi w niniejszej pracy, przekodowano zmienną odpowiedzi na binarne wartości liczbowe, gdzie wartość 1 oznaczała złośliwy, natomiast wartość 0 łagodny charakter wykrytego nowotworu.
	
	Dalsze przygotowania do procesu uczenia uwzględniały analizę i wykluczenie zmiennych skorelowanych. W tym celu posłużono się macierzą wykresów rozrzutu. Zdecydowano na wyłączenie z uczenia następujących predyktorów:
	
	\begin{enumerate}
		\item \textit{Log mean perimeter};
		\item \textit{Log mean area};
		\item \textit{Log mean symmetry};
		\item \textit{Arrhenius inverse std err perimeter};
		\item \textit{Arrhenius inverse std err area};
		\item \textit{Cube root std err concavity};
		\item \textit{Cube root mean largest perimeter};
		\item \textit{Log mean largest compactness};
		\item \textit{Cube root mean largest concavity};
		\item \textit{Mean largest concave points};
		\item \textit{Cube root mean largest radius};
		\item \textit{Cube root mean concave points};
		\item \textit{Cube root mean concavity};
		\item \textit{Log std err compactness};
		\item \textit{Log mean largest texture};
		\item \textit{Log mean largest smoothness};
		\item \textit{Cube root mean largest symmetry}.
	\end{enumerate}

	\begin{figure}[!ht]
		\centering
		\includegraphics[width=0.9\linewidth]{Rysunki/Rozdzial3/scatter}
		\caption{Macierz wykresów rozrzutu po usunięciu zmiennych skorelowanych}
		\label{scatter}
	\end{figure}
	
	Rysunek \ref{scatter} przedstawia wynikową macierz. Po usunięciu zmiennych skorelowanych, do procesu uczenia pozostało 9 predyktorów:
	
	\begin{enumerate}
		\item \textit{Log mean radius};
		\item \textit{Log mean texture};
		\item \textit{Log mean smoothness};
		\item \textit{Arrhenius inverse mean dimension};
		\item \textit{Arrhenius inverse std err radius};
		\item \textit{Log std err texture};
		\item \textit{Log std err smoothness};
		\item \textit{Arrhenius inverse std err symmetry};
		\item \textit{Arrhenius inverse std err dimension}.
	\end{enumerate}

	\subsection{Dane regresyjne}	
	\subsubsection{Analiza rozkładu danych}
	
	Podobnie jak w przypadku danych klasyfikacyjnych, analizę rozpoczęto od zapoznania się z rozkładem wybranej zmiennej odpowiedzi. Zauważono że posiada ona rozkład skrajnie prawostronny, co przedstawiono na rysunku \ref{fig:ferritin}.
	
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=0.5\linewidth]{Rozdzial3/ferritin}
		\caption{Wykres rozkładu zmiennej odpowiedzi dla zestawu regresyjnego.}
		\label{fig:ferritin}
	\end{figure}
	
	Na wykresie pudełkowym zawartym nad histogramem rozkładu można zauważyć wystąpienie dwóch obserwacji odstających. Podczas dalszej analizy, spostrzeżono podobny problem w przypadku zmiennych \textit{\%ISAF}, \textit{Iron}, \textit{sTfR}, \textit{Transferrin} oraz szczególnie \textit{Alpha GST}. Pozostałe zmienne charakteryzują się rozkładem zbliżonym do krzywej Gaussa, nie przejawiając obserwacji zaklasyfikowanych jako odstające. Rysunek \ref{fig:decision} przedstawia dwa przykładowe histogramy rozkładów zmiennych decyzyjnych.
	
	\begin{figure}[!ht]
		\begin{minipage}{0.40\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{Rozdzial3/decision}
			\caption{Przykładowy rozkład zmiennej decy- zyjnej (TIBC).}
			\label{fig:decision}		
		\end{minipage}%
		\hspace{0.10\textwidth}
		\begin{minipage}{0.40\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{Rozdzial3/decision1}
			\caption{Przykładowy rozkład zmiennej decy- zyjnej (ISAF).}
			\label{fig:decision1}			
		\end{minipage}
	\end{figure}
	
	Pojedyncza zmienna - \textit{Gender} - posiada dychotomiczny charakter, aczkolwiek okazuje się relatywnie bardzo zrównoważona, posiadając rozkład na poziomie przynależności w 46,7\% do klasy F reprezentującej kobiety oraz 53,3\% do klasy M odpowiadającej mężczyznom. Rozkład tej zmiennej przedstawiono na rys. \ref{fig:mnf}.
	
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=0.6\linewidth]{Rozdzial3/mnf}
		\caption{Rozkład zmiennej \textit{Gender}.}
		\label{fig:mnf}
	\end{figure}
	
	\subsubsection{Czyszczenie i normalizacja rozkładu danych}
	
	W trakcie przeglądu obserwacji, zauważono pojedynczą obserwację z brakującą wartością w przypadku zmiennej \textit{Ferritin}. Ze względu na wystąpienie tylko jednego takiego wpisu na 85 obserwacji, zdecydowano się na jej usunięcie. Pozostałymi kwestiami wymagającymi zaadresowania okazały się normalizacja rozkładu części zmiennych i decyzja o działaniu względem wartości odstających.
	
	W celu zmniejszenia liczby obserwacji odstających, postanowiono rozpocząć następny etap od problemu normalizacji rozkładu. Następujące zmienne, ze względu na ich obecną charakterystykę, nie zostały poddane żadnym przekształceniom:
	
	\begin{enumerate}
		\item \textit{Age};
		\item \textit{Gender};
		\item \textit{TIBC}.
	\end{enumerate}

	Dla pozostałych zmiennych decyzyjnych zastosowano trzy rodzaje transformacji opisanych w rozdz. 3.2.1.1. Pierwszą z nich była obliczenie logarytmu z wartości zmiennej, które przyniosło zadowalający efekt dla zmiennych:
	
	\begin{enumerate}
		\item \textit{alpha GST};
		\item \textit{pi GST};
		\item \textit{sTfR};
		\item \textit{Ferritin} (zmienna odpowiedzi).
	\end{enumerate}

	Rysunek \ref{fig:log2} przedstawia przykład uzyskanej zmiany rozkładu dla zmiennej odpowiedzi.
	
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=0.6\linewidth]{Rozdzial3/log2}
		\caption{Wpływ transformacji logarytmicznej na rozkład zmiennej odpowiedzi.}
		\label{fig:log2}
	\end{figure}

	Drugą transformacją o zadowalających wynikach okazało się zastosowanie pierwiastka sześciennego, co pokazano na rys. \ref{fig:cube2}. Wykorzystano ją do normalizacji rozkładu następujących zmiennych:
	
	\begin{enumerate}
		\item \textit{Transferrin};
		\item \textit{Iron};
		\item \textit{\%ISAF}.
	\end{enumerate}

	\begin{figure}
		\centering
		\includegraphics[width=0.6\linewidth]{Rozdzial3/cube2}
		\caption{Normalizacja rozkładu za pomocą transformacji pierwiastkiem kwadratowym.}
		\label{fig:cube2}
	\end{figure}

	Transformacja odwrotnym wzorem Arrheniusa okazała się nieskuteczna na wszystkich zmiennych, uzyskując gorsze efekty niż pozostałe przedstawione wyżej metody. Z racji na niewielką liczbę obserwacji, oraz stosunkowo małą liczbę wartości odstających, zdecydowano się na pozostawienie ich w procesie uczenia. W celu kompatybilności z omawianymi bibliotekami, przekodowano zmienną dychotomiczną na binarne wartości liczbowe, gdzie wartość 1 odpowiadała płci żeńskiej, natomiast 0 płci męskiej.

\section{Modele zbudowane w programie JMP}

W trakcie analizy zestawów danych wybrany został przedstawiony poniżej zestaw metod dla których wykonano i podsumowano testy praktyczne. Szablony struktury rozwiązań, takie jak np. wybór zmiennych uczestniczących w procesie uczenia, lub struktura sieci neuronowej zostały ustalone w sposób empiryczny z wykorzystaniem programu do uczenia maszynowego JMP. 




\subsection{Maszyna wektorów nośnych}

%Badanie zależności w modelu maszyny wektorów nośnych odbyło się z wykorzystaniem wykresu wpływu zmiennej decyzyjnej na zmienną odpowiedzi opartego o p-wartość. Jako poziom istotności testu o braku wypływu zmiennej na odpowiedź przyjęto 0,05. Rysunek \ref{fig:pvalue1} przedstawia w/w wykres wraz z p-wartościami dla poszczególnych zmiennych. Zauważyć można, że dla części zmiennych nie została wyznaczona p-wartość -- oznacza to, że część zmiennych jest ze sobą skorelowanych.

%Pierwszym krokiem w wybraniu istotnych zmiennych było usunięcie zmiennych skorelowanych, drugim natomiast stopniowe usuwanie zmiennych o p-wartości powyżej progu 0,05. Rysunek \ref{fig:pvalue2} przedstawia listę wraz z wykresem kolumnowym istotnych predyktorów. Ich lista, wraz z odpowiadającymi im p-wartościami została umieszczona w tabeli \ref{lin_reg:1}.

Do procesu uczenia maszyny wektorów nośnych wykorzystano wszystkie zmienne pozostałe po procesie czyszczenia i eksploracji danych. W celu walidacji użyto metody wybrania losowego zestawu walidacyjnego spośród dostarczonych danych, w proporcji 80\% obserwacji uczących i 20\% testowych, z użyciem wartości 1234 dla ziarna generatora liczb pseudolosowych. Jako funkcję jądra maszyny wektorów nośnych (ang. Support Vector Machine, SVM) wybrano jądro radialne (ang. \textit{Radial Basis Function}), która jest domyślnym wyborem dla SVM w środowisku JMP. 

%\begin{longtable}{l|c}
%	\centering
%	Nazwa zmiennej & p-wartość \\
%	\hline
	%\textit{Log mean largest texture} & 0,00000 \\
	%\textit{Log mean largest compactness} & 0,00000 \\
	%\textit{Cube root mean largest symmetry} & 0,00001 \\
	%\textit{Arrhenius inverse std err symmetry} & 0,00005 \\
	%\textit{Arrhenius inverse std err radius} & 0,00018 \\
	%\textit{Cube root mean concave points} & 0,00056 \\
	%\textit{Cube root mean largest concavity} & 0,00069 \\
	%\textit{Log std err texture} & 0,00252 \\
	%\textit{Cube root mean largest perimeter} & 0,00526 \\
	%\textit{Log mean smoothness} & 0,04867 \\
	%\textit{Log mean radius} & 0,04884 \\
	%\caption{Lista istotnych predyktorów}
%	\label{lin_reg:1}
%\end{longtable}

%\begin{figure}[!ht]
%	\centering
%	\includegraphics[width=0.7\linewidth]{Rysunki/Rozdzial3/pvalue1}
%	\caption{Zestawienie p-wartości dla zmiennych decyzyjnych.}
%	\label{fig:pvalue1}
%\end{figure}

%\begin{figure}[!ht]
%	\centering
%	\includegraphics[width=0.7\linewidth]{Rysunki/Rozdzial3/pvalue2}
%	\caption{Wykres p-wartości istotnych zmiennych decyzyjnych.}
%	\label{fig:pvalue2}
%\end{figure}

\begin{longtable}{l | c}
	\centering
	Zmienna decyzyjna & współczynnik przy predyktorze \\
	\hline
	\textit{Log mean radius} & 2,6191 \\
	\textit{Log mean texture} & 2,9353 \\
	\textit{Log mean smoothness} & -2,3502 \\
	\textit{Arrhenius inverse mean dimension} & 186640 \\
	\textit{Arrhenius inverse std err radius} & 38170 \\
	\textit{Log std err texture} & 0.1049 \\
	\textit{Log std err smoothness} & -5,0286 \\
	\textit{Arrhenius inverse std err symmetry} & 635100 \\
	\textit{Arrhenius inverse std err dimension} & 4053000 \\
	\caption{Wartości współczynników modelu przy predyktorach.}
	\label{svm:1}
\end{longtable} 

\begin{figure}[!ht]
	\begin{minipage}{0.48\textwidth}
		\centering
		\includegraphics[width=0.98\linewidth]{Rysunki/Rozdzial3/roc_svm1_test}
		\caption{Krzywa charakterystycz-na odbiornika dla danych uczących modelu SVM.}
		\label{fig:rocsvm1test}		
	\end{minipage}%
	\hspace{10pt}
	\begin{minipage}{0.48\textwidth}
		\centering
		\includegraphics[width=0.98\linewidth]{Rysunki/Rozdzial3/roc_svm1_val}
		\caption{Krzywa charakterystycz-na odbiornika dla danych walidacyjnych modelu SVM.}
		\label{fig:rocsvm1val}				
	\end{minipage}	
\end{figure}

Utworzony w ten sposób model uzyskał współczynnik błędnej klasyfikacji wynoszący 2,64\% dla danych testowych, oraz 9,57\% dla danych uczących. Tabela \ref{svm:1} przedstawia współczyniki wyliczone w procesie uczenia dla poszczególnych regresorów. Rysunki \ref{fig:rocsvm1test} oraz \ref{fig:rocsvm1val} przedstawiają krzywe charakterystyczne odbiornika dla uzyskanego modelu, pola pod którymi uzyskały wartość odpowiednio 0,9972 dla danych uczących i 0,9632 dla danych testowych.

\subsection{Regresja liniowa}

Podobnie jak w przypadku regresji logistycznej, do ustalenia które z regresorów mają największy wpływ na zmienną odpowiedzi, zastosowano wykres wpływu poszczególnych zmiennych oparty o p-wartość. Jako próg zaakceptowania parametru do procesu uczenia zdecydowano się na wybranie p-wartości wynoszących poniżej 0,05 jednostek. Rysunek \ref{fig:pvalue3} przedstawia wykres dla wszystkich zmiennych, natomiast rys. \ref{fig:pvalue4} ukazuje wykres zawierający jedynie wybrane zmienne. 

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.6\linewidth]{Rozdzial3/pvalue3}
	\caption{Wykres p-wartości dla wszystkich zmiennych.}
	\label{fig:pvalue3}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.7\linewidth]{Rozdzial3/pvalue4}
	\caption{Wykres p-wartości dla zmiennych wybranych do procesu uczenia.}
	\label{fig:pvalue4}
\end{figure}

W procesie eliminacji regresorów o p-wartości przekraczającej wybrany próg akceptacji, do procesu uczenia wybrano zmienne z tabeli \ref{pvalue}:

\begin{table}[!ht]
	\begin{minipage}{0.48\textwidth}
		\centering
		\begin{tabular}{l|c}
			Nazwa zmiennej & p-wartość \\
			\hline
			\textit{Gender} & 0.00002 \\
			\textit{TIBC} & 0.00066 \\
			\textit{Cube Root Iron} & 0.00667 \\
			\textit{Cube Root \%ISAF} & 0.01223 \\
			\textit{Age} & 0.02059
		\end{tabular}
		\caption{Wybrane zmienne decyzyjne i ich p-wartości.}
		\label{pvalue}
	\end{minipage}%
	\hspace{0.04\textwidth}
	\begin{minipage}{0.48\textwidth}
		\centering
		\begin{tabular}{l|c}
			Nazwa zmiennej & waga \\
			\hline
			\textit{Intercept} & 8,498959 \\
			\textit{Age} & 0.0201022 \\
			\textit{Gender[F - M]} & -0.959795 \\
			\textit{Cube Root Iron} & 2,9461861 \\
			\textit{TIBC} & -0.014182 \\
			\textit{Cube Root \%ISAF} & -4,356345
		\end{tabular}
		\caption{Wartości wag zmiennych decyzyjnych.}
		\label{weights}		
	\end{minipage}
\end{table}

W wyniku uczenia uzyskano model dla którego wartość współczynnika determinacji liniowej \textit{$R^2$} wyniosła 0.4654525, co sugeruje że dane są umiarkowanie dobrze aproksymowalne liniowo. Tabela \ref{weights} przedstawia nauczone wagi poszczególnych regresorów.