\chapter{Shark-ML}

\section{Introduction}

Shark-ML is a machine learning library dedicated for the C++ language. It has an open source and is distributed under the \textit{GNU Lesser General Public License}. The main focus of this library are linear and nonlinear optimization problems (as such containing parts of linear algebra library functionalities), kernel machines (eg. support vector machine) and neural networks \cite{shark}. The entity that provides the library is the University of Copenhagen in Denmark, and Neuroinformatics institute of Ruhr-Universitat Bochum in Germany. 

\section{Data sources formats}

The library operates on its own representations of matrices and vectors, which are created by wrapping raw arrays using specialized adapters, such as eg. \textit{remora::dense-\char`_matrix\char`_adaptor<>()}, or by using containers from the C++ standard library and \textit{createDataFromRange()} function. This mechanism is the exact same as in case of other libraries mentioned in this dissertation, which gives the user a lot of freedom in terms of what format the data is stored in, and how it's read. The library also contains a dedicated parser for files in the CSV format, however it allows the presence of only numeric data within the file. In order to use it, it is required to create a \textit{ClassificationDataset} or \textit{RegressionDataset} container and the \textit{importCSV()} method, which stores the read data in the aforementioned container object via the return-by-parameter mechanism. One of the arguments of importCSV() method describes which of the columns contain labels, thanks to which the library is able to instantly separate the predictors from labels. The article \cite{shark:http} available on GitHub shows also how to download data from the internet using \textit{curl} library API, and how to process it into the form acceptable by Shark-ML. Current version of the library also contains some in-built functions that support downloading data via the HTTP protocol. Listing \ref{shark:csv} shows how to read data from .csv file that is present on the user's hard drive.

\newpage
\cppcode{Result/inc/shark/csv.hpp}{Reading from CSV file in Shark-ML.}{shark:csv}

In order to wrap the data contained within C++ standard library containers into objects accepted by the Shark-ML, it is necessary to use the aforementioned adaptor functions which accept the pointer for the data in raw array contained within the container together with expected size of target matrix or vector. The wrapping method was shown on listing \ref{shark:adaptor}. 

\cppcode{Rozdzial5/shark-adaptor.cpp}{Data wrapping method in Shark-ML\cite{handsOnMachineLearning}.}{shark:adaptor}

\section{Data exploration and processing methods}

\subsection{Normalization}

Shark-ML implements normalization as training class for model \textit{Normalizer}, providing three usable classes to the user:

\begin{itemize}
	\item \textit{NormalizeComponentsUnitInterval} - processes data as if they are contained within the unit interval;
	\item \textit{NormalizeComponentsUnitVariance} - recalculates data to achieve unit variance, and sometimes also the mean equal to 0.
	\item \textit{NormalizeComponentsWhitening} - data is processed in a way that guarantees mean equal to 0 and user-provided variance (unit variance by default).
\end{itemize}

Those classes base on using the \textit{train()} method on the normalizer object, in order to configure it to process both testing data, and any other data the user wants to introduce into the model. Their additional functionality is data shuffling and separating a validation dataset using \textit{shuffle()} and \textit{splitAtElement()} methods of the \textit{ClassificationDataset} object. Listing \ref{shark:preprocessing} shows an example of initial data processing using normalization.


\cppcode{Result/inc/shark/normalization.hpp}{Initial training data preprocessing in Shark-ML\cite{shark:http}.}{shark:preprocessing}

\subsection{Dimensionality reduction}
\subsubsection{Principal component analysis}

Teh dimensionality reduction algorithm utilizing principal component analysis is implemented in Shark as \textit{PCA} class. It uses a linear model as an encoder and accepts target data dimension in the \textit{encoder()} method. The result of this function call is configuring the linear model in a way that allows creating datasets with reduced dimensions. Listing \ref{shark:pca} shows use case of the \textit{PCA} class.

\cppcode{Rozdzial5/shark-dimension-reduction.cpp}{Dimensionality reduction using \textit{PCA} class and an encoder in Shark-ML.}{shark:pca}

\subsubsection{Linear discriminant analysis}

In case of Shark-ML library, the linear discriminant analysis (\textit{LDA}) is based on finding an analytical solution via configuration of \textit{LinearClassifier} model via \textit{LDA} training class, using the \textit{train()} method. It is also possible to use the same model for classification task, receiving predictions for a data set by calling the linear classifier functor (use of operator()) providing data from \textit{ClassificationDataset} acquired by calling \textit{inputs()} method. Detailed implementation on the dimensionality reduction of data using this method are presented on listing \ref{shark:lda-red}.

\cppcode{Rozdzial5/shark-lda-red.cpp}{Dimensionality reduction example using LDA model in Shark-ML\cite{handsOnMachineLearning}.}{shark:lda-red}

\subsection{L1 regularization}

The Shark library, in contrary to the Shogun library does not contain specifically defined mechanisms for standalone regularization in the models. Instead, there is a possibility to put a regularization object in the trainer object, using the \textit{setREgularization()} function. In order to use the Lasso method, the user needs to put the \textit{shark::OneNormRegularizer} object within the trainer of choice, and next carry out the teaching process.

\subsection{L2 regularization}

Similarly to the Lasso method, using L2 regularization in the trained model is based on providing the right regularizer object to the trainer object. For L2 method it is the \textit{shark::TwoNormRegularizer} class.

\section{Machine learning models}

\subsection{Linear regression}

One of the fundamental models offered by this library is the linear regression. In order to use it, the \textit{LinearModel} class is provided, which offers an analytical solution using the \textit{LinearRegression} trainer class, or iterative approach implemented by the \textit{LineatSAGTrainer} class, which utilizes the statistic average gradient method. In case of more complicated regression models, where the analytical solution might not exist, the iterative method can be used with the optimizer chose by the user. The whole method comes down into teaching the optimizer using a loss function, and subsequently loading the acquired weights into the regression model. Parameters of the model can be read by calling the \textit{offset()}, \textit{matrix()} or \textit{parameterVector()} methods. Listing \ref{shark:linear} shows the iterative approach, and listing \ref{shark:linear2} presents the analytical method. 

\cppcode{Result/inc/shark/linear.hpp}{Example of linear regression using SAG - Shark-ML.}{shark:linear}

\cppcode{Rozdzial5/shark-linear2.cpp}{Example of linear regression with analytical method - Shark-ML\cite{handsOnMachineLearning}.}{shark:linear2}

\subsection{Logistic regression}

The logistic regression mechanism available in the Shark-ML library by its nature solves a binary classification problem. There is however possibility to use multiple classifiers, which count is described by the equation:

\begin{equation}
	\frac{N(N-1)}{2}	
	\label{multiclass}
\end{equation}

where $N$ designates the number of classes present in the problem. Created classifiers are then combined into one object using the right configuration of \textit{OneVersusOneClassifier} class, which then allows for solving a multiclass classification. In order to achieve this, the dataset needs to be iteratively split into binary characteristic subproblems using the in-built \textit{binarySubProblem()} method, which accepts the data set and desired classes. Teaching process is carried out using the \textit{LogisticRegression} trainer object. After the submodel batch is trained, they are loaded into the primary model. The final use of the ready multiclass classifier is no different than using a model acquired eg. in linear classification. Listing \ref{shark:logistic} describes the function for building a multiclass classifier, while listing \ref{shark:logistic2} presents the creation of simple binary classification model. 

\cppcode{Rozdzial5/shark-logistic.cpp}{Example of multiclass classifier building - Shark\cite{handsOnMachineLearning}.}{shark:logistic}

\cppcode{Result/inc/shark/logistic.hpp}{Example of simple binary linear regression - Shark-ML.}{shark:logistic2}

\subsection{Support vector machine}

One of the very important from the perspective of Shark-ML library use machine learning methods provided by it is the support vector machines, which is a type of kernel model. It is based on performing linear regression inside the space of characteristics derived from the used kernel. Similarly to the logistic regression case, the API of the library allows directly only for binary classification, however using it to solve a multiclass problem requires a combination of many dichotomous support vector machines into a compound model, which can be achieved using \textit{OneVersusOneClassifier} and requires the submodels count described by the \ref{multiclass} equation. Consistently with the characteristic trait of this library, the use of the method is split into creating an instance of the model and a trainer object, which configures the model in the training process. The user is presented with following classes:

\begin{itemize}
	\item \textit{GaussianRbfKernel} - it is responsible for calculating the similarity between provided characteristics using a radial basis function;
	\item \textit{KernelClassifier} - it carries out the linear regression within the space provided by the kernel;
	\item \textit{CSvmTrainer} - trainer class which performs the training in accordance with configured parameters;
\end{itemize}

The user can configure the model with following parameters:

\begin{itemize}
	\item model throughput - provided to the \textit{GaussianRbfKernel} constructor as a number from the interval of $[0; 1]$;
	\item regularization - a real number set in the constructor of \textit{CSvmTrainer}. By default the support vector machines uses L1 norm as a punishment for crossing the desired boundary;
	\item bias - a binary flag (bool) describing whether the model is supposed to the bias term, set in the \textit{CSvmTrainer} constructor;
	\item \textit{sparsify} - a parameter stating whether the model should keep the non-support vectors, available via the \textit{sparsify()} trainer method;
	\item minimal precision for finishing the training - it allows the user to specify the satisfying precision of the model. It is available as a member of the \textit{stoppingCondition()} structure for the trainer;
	\item cache size - set via the \textit{setCacheSize()} method of the trainer;
\end{itemize}

The use method of the model is identical to the other models, via calling the operator() member function. Listing \ref{shark:svm} shows an example of creating and configuring the model based on the available documentation of the library, however listing \ref{shark:svm2} shows the creating of support vector machine for multiclass problems inside a function that accepts training and validation datasets.

\cppcode{Result/inc/shark/svm.hpp}{Example of binary support vector machine - Shark-ML.}{shark:svm}

\cppcode{Rozdzial5/shark-svm2.cpp}{Example of multiclass support vector machine - Shark\cite{handsOnMachineLearning}.}{shark:svm2} 

\subsection{K-nearest neighbors algorithm}

One of the classification methods provided by the Shark-ML library is the nearest neighbor model which can be equipped in various algorithms, one of which is the K-nearest neighbors algorithm (kNN). The model is represented by the 
\textit{NEarestNeighborModel} class. The library allows for using a brute-force solution, or a solution based on space partition trees, using the \textit{KDTree} class and \textit{TreeNearestNeighbors}. In contrary to previously shown methods, performing a multiclass classification in this case does not require compound models or providing the number of classes to the model. It is automatically configured based on the provided training data. Listing \ref{shark:knn} shows how to prepare the kNN classifier. 

\cppcode{Result/inc/shark/knn.hpp}{Example of kNN classifier - Shark-ML.}{shark:knn}

\subsection{Ensemble algorithm}

Besides commonly known algorithms, the Shark-ML library also provides more complex structures, such as eg. ensemble algorithm models, which bases on using many component algorithms based on fragments on the characteristics space, in order to combine acquired results to increase the predictions precision. Unfortunately, the only mechanism present in this library that utilizes this technique is random forest composed of decision trees, allowing only for solving classification problems (it does not allow for using it for regression purposes). A classical approach for this library is taken, where the implementation is performed by creating a trainer object - in this case of \textit{RFTrainer} class - which allows for configuring the parameters and subsequently training the model represented by the \textit{RFClassifier} object. Besired the random forest algorithm, there is a possibility of using this library for creating a model in the stacking technique, however due to the fact that this option does not occur natively, it is outside of this dissertation scope. Listing \ref{shark:rf} shows how to create and use the random forest model. 

\cppcode{Rozdzial5/shark-rf.cpp}{Creation of ensemble random forest model - Shark-ML\cite{handsOnMachineLearning}.}{shark:rf}

\subsection{Neural network}

Construction of neural network using the Shark-ML library utilizes some mechanisms offered by the \textit{LinearModel<>} class. It allows for deciding the type and number of inputs, outputs and whether to use the bias term. Each layer consists of singular linear model, where the number of inputs describes the neurons count within the layer. Neuron activation function configuration happens during passing the data types to the model template. Full list of activation functions can be found in the library documentation \cite{shark:activation}. The next step is to prepare an object of \textit{ErrorFunction<>} class based on one of the available loss functions, which will be configured for use by optimizer which carries out the training process. After preparing the loss function, it needs to be initialized with random weights and the user needs to create and configure an optimizer object of choice. At this stage, the network is ready for training. The process is based around iteratively executing a \textit{step()} function of the optimizer. Due to the need of using a user-defined loop, there is a possibility for describing user's own stop conditions to be evaluated in each epoch, such as epoch count or passing a certain threshold by the loss function. Inside the outside epoch loop, another loop needs to be placed that will go through each data batches, performing the optimizer step on them. After the training is completed, the model needs to be configured by passing the weights acquired by the optimizer to it. As a result, user gets a fully set up neural network model. Listing \ref{shark:neural} shows the code snippets performing the whole process, which is an example from the handbook \cite{handsOnMachineLearning}.

\cppcode{Result/inc/shark/neural.hpp}{Example of neural network with two hidden layers for classification task - Shark\cite{handsOnMachineLearning}.}{shark:neural}

\section{Model analysis methods}

\subsection{Loss functions}

Shark-ML library offers a range of loss functions allowing for proper verification of model accuracy. Some of them are\cite{shark:loss}: 

\begin{itemize}
	\item \textbf{mean absolute error} - available as \textit{AbsoluteLoss} class;
	\item \textbf{mean squared error} - available as \textit{SquaredLoss} class;
	\item \textbf{zero-one loss} - available as \textit{ZeroOneLoss} class;
	\item \textbf{discrete loss} - available as \textit{DiscreteLoss} class;
	\item \textbf{cross entropy} - available as \textit{CrossEntropy} class;
	\item \textbf{hinge loss} - available as \textit{HingeLoss} class;
	\item \textbf{squared hinge loss} - available as \textit{SquaredHingeLoss} class;
	\item \textbf{epsilon hinge loss} - available as \textit{EpsilonHingeLoss} class;
	\item \textbf{squared epsilon hinge loss} - available as \textit{SquaredEpsilonHingeLoss} class;
	\item \textbf{Huber loss} - available as \textit{HuberLoss} class;
	\item \textbf{Tukey biweight loss} - available as \textit{TukeyBiweightLoss} class.
\end{itemize}

Each of the aforementioned classes is used in a schematic way, through the creation of the loss class object of choice, and subsequently calling it as a function, passing expected values and acquired predictions. Listing \ref{shark:mse} presents the way to use a loss function via an example of mean squared error.

\cppcode{Rozdzial5/shark-mse.cpp}{Mean squared error loss function use - Shark-ML.}{shark:mse}

\subsection{$R^2$ and adjusted $R^2$ metrics}

Shark-ML does not implement direct representation of the $R^2$ metric, unlike the loss functions. However, it provides a function to calculate the variance of given data, which allows for fairly easy manual implementation of both metrics. Listing \ref{shark:r2} shows how to calculate them, given the value of mean squared error is known.

\cppcode{Rozdzial5/shark-r2.cpp}{Implementation of $R^2$ and adjusted $R^2$ metrics - Shark-ML.}{shark:r2}

\subsection{ROC curve}

The ROC curve is one of commonly used metrics for evaluating model's correctness, and as such, it could not be missed in the Shark-ML library. It is available under the \textit{NegativeAUC} class, which can be used similarly to the loss functions. In contrary to the classic approach, this class calculates the compliment of the ROC curve, so it can be used as a minimized goal in the training process. Listing \ref{shark:roc} shows how to use it to calculate the mentioned metric.

\cppcode{Result/inc/shark/printEvaluation2.hpp}{Example of calculating ROC curve metric - Shark-ML.}{shark:roc} 

\subsection{K-fold cross validation}

The process of acquiring the best hyperparameters value in Shark-ML consists of performing multiple internal training processes of given model, however it focuses on comparing the results, and because of that, its description was placed in this paragraph. In order to use the K-fold cross validation implementation provided by the library, three classes are required. First of them is \textit{CVFolds}, which is tasked with storing data that is split into a number of segments. The second one is \textit{CrossValidationError} - a template accepting model type, for which the validation error will be calculated, and an object of the loss function, which will perform the calculation. The last but not least is \textit{GridSearch}, which iteratively selects fragments for the training process and hyperparameters for the model. This procedure results in acquiring the best set of hyperparameters for training of the model. User needs to invoke the \textit{step()} method of the \textit{GridSearch} object only once. Listing \ref{shark:cv} shows an example of this mechanism from the handbook\cite{handsOnMachineLearning}, where the author uses the aforementioned classes with manually implemented polynomial regression model.  

\cppcode{Rozdzial5/shark-cv.cpp}{Example of K-fold cross validation in Shark-ML\cite{handsOnMachineLearning}.}{shark:cv}

\section{Documentation and sources availability}

The Shark-ML library has a shortened documentation available on the main website of the project, alongside example source code files added to the repository. It is also mentioned in the handbook \textit{"Hands-On Machine Learning with C++"}\cite{handsOnMachineLearning}, which shows examples of using selected functionalities. What makes it stand out from all the libraries described in this dissertation however is the fact, that it is dedicated for the C++ language, and as such, community threads and articles describing creation of various types of models with it, and sample source codes are way more available. The main page documentation is clear and extensive enaugh to make using the available API way easier.