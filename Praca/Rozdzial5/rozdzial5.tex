\chapter{Shark-ML}

\section{Introduction}

Shark-ML is a machine learning library dedicated for the C++ language. It has an open source and is distributed under the \textit{GNU Lesser General Public License}. The main focus of this library are linear and nonlinear optimization problems (as such containing parts of linear algebra library functionalities), kernel machines (eg. support vector machine) and neural networks \cite{shark}. The entity that provides the library is the University of Copenhagen in Denmark, and Neuroinformatics institute of Ruhr-Universitat Bochum in Germany. 

\section{Data sources formats}

The library operates on its own representations of matrices and vectors, which are created by wrapping raw arrays using specialized adapters, such as eg. \textit{remora::dense-\char`_matrix\char`_adaptor<>()}, or by using containers from the C++ standard library and \textit{createDataFromRange()} function. This mechanism is the exact same as in case of other libraries mentioned in this dissertation, which gives the user a lot of freedom in terms of what format the data is stored in, and how it's read. The library also contains a dedicated parser for files in the CSV format, however it allows the presence of only numeric data within the file. In order to use it, it is required to create a \textit{ClassificationDataset} or \textit{RegressionDataset} container and the \textit{importCSV()} method, which stores the read data in the aforementioned container object via the return-by-parameter mechanism. One of the arguments of importCSV() method describes which of the columns contain labels, thanks to which the library is able to instantly separate the predictors from labels. The article \cite{shark:http} available on GitHub shows also how to download data from the internet using \textit{curl} library API, and how to process it into the form acceptable by Shark-ML. Current version of the library also contains some in-built functions that support downloading data via the HTTP protocol. Listing \ref{shark:csv} shows how to read data from .csv file that is present on the user's hard drive.

\newpage
\cppcode{Result/inc/shark/csv.hpp}{Reading from CSV file in Shark-ML.}{shark:csv}

In order to wrap the data contained within C++ standard library containers into objects accepted by the Shark-ML, it is necessary to use the aforementioned adaptor functions which accept the pointer for the data in raw array contained within the container together with expected size of target matrix or vector. The wrapping method was shown on listing \ref{shark:adaptor}. 

\cppcode{Rozdzial5/shark-adaptor.cpp}{Data wrapping method in Shark-ML\cite{handsOnMachineLearning}.}{shark:adaptor}

\section{Data exploration and processing methods}

\subsection{Normalization}

Shark-ML implements normalization as training class for model \textit{Normalizer}, providing three usable classes to the user:

\begin{itemize}
	\item \textit{NormalizeComponentsUnitInterval} - processes data as if they are contained within the unit interval;
	\item \textit{NormalizeComponentsUnitVariance} - recalculates data to achieve unit variance, and sometimes also the mean equal to 0.
	\item \textit{NormalizeComponentsWhitening} - data is processed in a way that guarantees mean equal to 0 and user-provided variance (unit variance by default).
\end{itemize}

Those classes base on using the \textit{train()} method on the normalizer object, in order to configure it to process both testing data, and any other data the user wants to introduce into the model. Their additional functionality is data shuffling and separating a validation dataset using \textit{shuffle()} and \textit{splitAtElement()} methods of the \textit{ClassificationDataset} object. Listing \ref{shark:preprocessing} shows an example of initial data processing using normalization.


\cppcode{Result/inc/shark/normalization.hpp}{Initial training data preprocessing in Shark\cite{shark:http}.}{shark:preprocessing}

\subsection{Dimensionality reduction}
\subsubsection{Principal component analysis}

Teh dimensionality reduction algorithm utilizing principal component analysis is implemented in Shark as \textit{PCA} class. It uses a linear model as an encoder and accepts target data dimension in the \textit{encoder()} method. The result of this function call is configuring the linear model in a way that allows creating datasets with reduced dimensions. Listing \ref{shark:pca} shows use case of the \textit{PCA} class.

\cppcode{Rozdzial5/shark-dimension-reduction.cpp}{Dimensionality reduction using \textit{PCA} class and an encoder in Shark.}{shark:pca}

\subsubsection{Linear discriminant analysis}

In case of Shark-ML library, the linear discriminant analysis (\textit{LDA}) is based on finding an analytical solution via configuration of \textit{LinearClassifier} model via \textit{LDA} training class, using the \textit{train()} method. It is also possible to use the same model for classification task, receiving predictions for a data set by calling the linear classifier functor (use of operator()) providing data from \textit{ClassificationDataset} acquired by calling \textit{inputs()} method. Detailed implementation on the dimensionality reduction of data using this method are presented on listing \ref{shark:lda-red}.

\cppcode{Rozdzial5/shark-lda-red.cpp}{Dimensionality reduction example using LDA model in Shark\cite{handsOnMachineLearning}.}{shark:lda-red}

\subsection{L1 regularization}

The Shark library, in contrary to the Shogun library does not contain specifically defined mechanisms for standalone regularization in the models. Instead, there is a possibility to put a regularization object in the trainer object, using the \textit{setREgularization()} function. In order to use the Lasso method, the user needs to put the \textit{shark::OneNormRegularizer} object within the trainer of choice, and next carry out the teaching process.

\subsection{L2 regularization}

Similarly to the Lasso method, using L2 regularization in the trained model is based on providing the right regularizer object to the trainer object. For L2 method it is the \textit{shark::TwoNormRegularizer} class.

\section{Machine learning models}

\subsection{Linear regression}

One of the fundamental models offered by this library is the linear regression. In order to use it, the \textit{LinearModel} class is provided, which offers an analytical solution using the \textit{LinearRegression} trainer class, or iterative approach implemented by the \textit{LineatSAGTrainer} class, which utilizes the statistic average gradient method. In case of more complicated regression models, where the analytical solution might not exist, the iterative method can be used with the optimizer chose by the user. The whole method comes down into teaching the optimizer using a loss function, and subsequently loading the acquired weights into the regression model. Parameters of the model can be read by calling the \textit{offset()}, \textit{matrix()} or \textit{parameterVector()} methods. Listing \ref{shark:linear} shows the iterative approach, and listing \ref{shark:linear2} presents the analytical method. 

\cppcode{Result/inc/shark/linear.hpp}{Example of linear regression using SAG - Shark.}{shark:linear}

\cppcode{Rozdzial5/shark-linear2.cpp}{Example of linear regression with analytical method - Shark\cite{handsOnMachineLearning}.}{shark:linear2}

\subsection{Logistic regression}

The logistic regression mechanism available in the Shark-ML library by its nature solves a binary classification problem. There is however possibility to use multiple classifiers, which count is described by the equation:

\begin{equation}
	\frac{N(N-1)}{2}	
	\label{multiclass}
\end{equation}

where $N$ designates the number of classes present in the problem. Created classifiers are then combined into one object using the right configuration of \textit{OneVersusOneClassifier} class, which then allows for solving a multiclass classification. In order to achieve this, the dataset needs to be iteratively split into binary characteristic subproblems using the in-built \textit{binarySubProblem()} method, which accepts the data set and desired classes. Teaching process is carried out using the \textit{LogisticRegression} trainer object. After the submodel batch is trained, they are loaded into the primary model. The final use of the ready multiclass classifier is no different than using a model acquired eg. in linear classification. Listing \ref{shark:logistic} describes the function for building a multiclass classifier, while listing \ref{shark:logistic2} presents the creation of simple binary classification model. 

\cppcode{Rozdzial5/shark-logistic.cpp}{Example of multiclass classifier building - Shark\cite{handsOnMachineLearning}.}{shark:logistic}

\cppcode{Result/inc/shark/logistic.hpp}{Example of simple binary linear regression - Shark.}{shark:logistic2}

\subsection{Support vector machine}

One of the very important from the perspective of Shark-ML library use machine learning methods provided by it is the support vector machines, which is a type of kernel model. It is based on performing linear regression inside the space of characteristics derived from the used kernel. Similarly to the logistic regression case, the API of the library allows directly only for binary classification, however using it to solve a multiclass problem requires a combination of many dichotomous support vector machines into a compound model, which can be achieved using \textit{OneVersusOneClassifier} and requires the submodels count described by the \ref{multiclass} equation. Consistently with the characteristic trait of this library, the use of the method is split into creating an instance of the model and a trainer object, which configures the model in the training process. The user is presented with following classes:

\begin{itemize}
	\item \textit{GaussianRbfKernel} - it is responsible for calculating the similarity between provided characteristics using a radial basis function;
	\item \textit{KernelClassifier} - it carries out the linear regression within the space provided by the kernel;
	\item \textit{CSvmTrainer} - trainer class which performs the training in accordance with configured parameters;
\end{itemize}

The user can configure the model with following parameters:

\begin{itemize}
	\item model throughput - provided to the \textit{GaussianRbfKernel} constructor as a number from the interval of $[0; 1]$;
	\item regularization - a real number set in the constructor of \textit{CSvmTrainer}. By default the support vector machines uses L1 norm as a punishment for crossing the desired boundary;
	\item bias - a binary flag (bool) describing whether the model is supposed to the bias term, set in the \textit{CSvmTrainer} constructor;
	\item \textit{sparsify} - a parameter stating whether the model should keep the non-support vectors, available via the \textit{sparsify()} trainer method;
	\item minimal precision for finishing the training - it allows the user to specify the satisfying precision of the model. It is available as a member of the \textit{stoppingCondition()} structure for the trainer;
	\item cache size - set via the \textit{setCacheSize()} method of the trainer;
\end{itemize}

The use method of the model is identical to the other models, via calling the operator() member function. Listing \ref{shark:svm} shows an example of creating and configuring the model based on the available documentation of the library, however listing \ref{shark:svm2} shows the creating of support vector machine for multiclass problems inside a function that accepts training and validation datasets.

\cppcode{Result/inc/shark/svm.hpp}{Example of binary support vector machine - Shark.}{shark:svm}

\cppcode{Rozdzial5/shark-svm2.cpp}{Example of multiclass support vector machine - Shark\cite{handsOnMachineLearning}.}{shark:svm2} 

\subsection{K-nearest neighbors algorithm}

One of the classification methods provided by the Shark-ML library is the nearest neighbor model which can be equipped in various algorithms, one of which is the K-nearest neighbors algorithm (kNN). The model is represented by the 
\textit{NEarestNeighborModel} class. The library allows for using a brute-force solution, or a solution based on space partition trees, using the \textit{KDTree} class and \textit{TreeNearestNeighbors}. In contrary to previously shown methods, performing a multiclass classification in this case does not require compound models or providing the number of classes to the model. It is automatically configured based on the provided training data. Listing \ref{shark:knn} shows how to prepare the kNN classifier. 

\cppcode{Result/inc/shark/knn.hpp}{Example of kNN classifier - Shark.}{shark:knn}

\subsection{Ensemble algorithm}

Besides commonly known algorithms, the Shark-ML library also provides more complex structures, such as eg. ensemble algorithm models, which bases on using many component algorithms based on fragments on the characteristics space, in order to combine acquired results to increase the predictions precision. Unfortunately, the only mechanism present in this library that utilizes this technique is random forest composed of decision trees, allowing only for solving classification problems (it does not allow for using it for regression purposes). A classical approach for this library is taken, where the implementation is performed by creating a trainer object - in this case of \textit{RFTrainer} class - which allows for configuring the parameters and subsequently training the model represented by the \textit{RFClassifier} object. Besired the random forest algorithm, there is a possibility of using this library for creating a model in the stacking technique, however due to the fact that this option does not occur natively, it is outside of this dissertation scope. Listing \ref{shark:rf} shows how to create and use the random forest model. 

\cppcode{Rozdzial5/shark-rf.cpp}{Creation of ensemble random forest model - Shark\cite{handsOnMachineLearning}.}{shark:rf}

\subsection{Neural network}

Construction of neural network using the Shark-ML library utilizes some mechanisms offered by the \textit{LinearModel<>} class. It allows for deciding the type and number of inputs, outputs and whether to use the bias term. Each layer consists of singular linear model, where the number of inputs describes the neurons count within the layer. Neuron activation function configuration happens during passing the data types to the model template. Full list of activation functions can be found in the library documentation \cite{shark:activation}. The next step is to prepare an object of \textit{ErrorFunction<>} class based on one of the available loss functions, which will be configured for use by optimizer which carries out the training process. After preparing the loss function, it needs to be initialized with random weights and the user needs to create and configure an optimizer object of choice. At this stage, the network is ready for training. The process is based around iteratively executing a \textit{step()} function of the optimizer. Due to the need of using a user-defined loop, there is a possibility for describing user's own stop conditions to be evaluated in each epoch, such as epoch count or passing a certain threshold by the loss function. Inside the outside epoch loop, another loop needs to be placed that will go through each data batches, performing the optimizer step on them. After the training is completed, the model needs to be configured by passing the weights acquired by the optimizer to it. As a result, user gets a fully set up neural network model. Listing \ref{shark:neural} shows the code snippets performing the whole process, which is an example from the handbook \cite{handsOnMachineLearning}.

\cppcode{Result/inc/shark/neural.hpp}{Example of neural network with two hidden layers for classification task - Shark\cite{handsOnMachineLearning}.}{shark:neural}

\section{Model analysis methods}

\subsection{Funkcje straty}

Biblioteka Shark-ML oferuje szereg funkcji straty pozwalających na wymierną weryfikację dokładności modelu. Należą do nich \cite{shark:loss}:

\begin{itemize}
	\item \textbf{średni błąd bezwzględny} - realizowany za pomocą klasy \textit{AbsoluteLoss};
	\item \textbf{błąd średniokwadratowy} - realizowany za pomocą klasy \textit{SquaredLoss};
	\item \textbf{błąd typu zero-jeden} - realizowany za pomocą klasy \textit{ZeroOneLoss};
	\item \textbf{błąd dyskretny} - realizowany za pomocą klasy \textit{DiscreteLoss};
	\item \textbf{entropia krzyżowa} - realizowana za pomocą klasy \textit{CrossEntropy};
	\item \textbf{kawałkami liniowa funkcja straty} - realizowany za pomocą klasy \textit{HingeLoss};
	\item \textbf{średniokwadratowy błąd kawałkami liniowej funkcji straty} - realizowany za pomocą klasy \textit{SquaredHingeLoss};
	\item \textbf{kawałkami liniowa funkcja straty typu epsilon} - realizowany za pomocą klasy \textit{EpsilonHingeLoss};
	\item \textbf{średniokwadratowy błąd kawałkami liniowej funkcji straty typu epsilon} - realizowany za pomocą klasy \textit{SquaredEpsilonHingeLoss};
	\item \textbf{funkcja straty Hubera} - realizowana za pomocą klasy \textit{HuberLoss};
	\item \textbf{funkcja straty Tukeya} - realizowana za pomocą klasy \textit{TukeyBiweightLoss}.
\end{itemize}

Każda z powyższych klas używana jest w schematyczny sposób, poprzez wcześniejsze utworzenie obiektu klasy wybranej funkcji straty, a następnie wywołanie jej jako funkcji przekazując wartości oczekiwane oraz otrzymane predykcje modelu. Listing \ref{shark:mse} przedstawia omówiony sposób użycia na przykładzie błędu średniokwadratowego.

\cppcode{Rozdzial5/shark-mse.cpp}{Użycie funkcji straty na przykładzie błędu średniokwadratowego}{shark:mse}

\subsection{Metryka $R^2$ i skorygowane $R^2$}

Biblioteka Shark-ML nie oferuje bezpośredniej klasy reprezentującej metrykę $R^2$ jak w przypadku funkcji strat, jednak udostępnia użytkownikowi funkcję obliczania wariancji danych, co umożliwia bardzo łatwą samodzielną implementację obu metryk. Listing \ref{shark:r2} przedstawia sposób ich wyliczenia, posiadając wartość błędu średniokwadratowego.

\cppcode{Rozdzial5/shark-r2.cpp}{Implementacja metryk $R^2$ oraz skorygowanego $R^2$.}{shark:r2}

\subsection{Pole pod wykresem krzywej charakterystycznej odbiornika}

Pole pod wykresem krzywej charakterystycznej odbiornika stanowi jedną z często wykorzystywanych metryk poprawności predykcji modelu, w związku z czym nie mogło jej zabraknąć w bibliotece Shark-ML. Jest ona dostępna za pośrednictwem klasy \textit{NegativeAUC}, wykorzystywanej w taki sam sposób jak pozostałe omówione wcześniej funkcje straty. W przeciwieństwie do standardowego podejścia, wspomniana klasa oblicza dopełnienie pola pod wykresem funkcji ROC, aby umożliwić wykorzystanie jej jako minimalizowanego celu w procesie uczenia. Listing \ref{shark:roc} przedstawia sposób obliczenia wartości wymienionej metryki.

\cppcode{Result/inc/shark/printEvaluation2.hpp}{Przykład obliczenia pola pod wykresem funkcji ROC dla Shark-ML.}{shark:roc} 

\subsection{K-krotny sprawdzian krzyżowy}

Proces poszukiwania najlepszych wartości hiperparametrów w Shark-ML uwzględnia przeprowadzenie wewnętrznie uczenia danego modelu, lecz skupia się na porównaniu uzyskiwanych wyników, w związku z czym jego opis zamieszczony został w tej sekcji. Użycie implementacji metody K-krotnego sprawdzianu krzyżowego w wymaga wykorzystania trzech klas. Pierwszą z nich stanowi \textit{CVFolds}, której zadaniem jest przechowanie zestawu danych podzielonego na odpowiednią liczbę fragmentów. Drugą jest klasa \textit{CrossValidationError} stanowiąca szablon przyjmujący typ modelu, dla którego określany będzie błąd walidacji, oraz obiekt klasy błędu, który ma zostać wyliczony. Ostatnią klasą jest \textit{GridSearch}, którego zadaniem jest iteracyjny wybór fragmentów do uczenia i wyznaczenie wartości hiperparametrów dla modelu. Wynikiem procesu jest uzyskanie najlepszego zestawu hiperparametrów do procedury uczenia - użytkownik musi zawołać metodę \textit{step()} klasy \textit{GridSearch} tylko jeden raz. Listing \ref{shark:cv} przedstawia przykład zawarty w podręczniku \cite{handsOnMachineLearning}, w którym autor przedstawia proces wykorzystania powyższych klas na własnoręcznie zaimplementowanym modelu regresji wielomianowej. 

\cppcode{Rozdzial5/shark-cv.cpp}{Przykład realizacji sprawdzianu krzyżowego K-fold w Shark-ML \cite{handsOnMachineLearning}.}{shark:cv}

\section{Dostępność dokumentacji i źródeł wiedzy}

Biblioteka Shark-ML posiada skróconą dokumentację dostępną na głównej stronie internetowej projektu, wraz z przykładowymi plikami źródłowymi dołączonymi do repozytorium. Jest ona także wspomniana w książce ,,Hands-On Machine Learning with C++'', przedstawiającej sposoby użycia wybranych funkcjonalności. Kwestią wyróżniającą ją natomiast na tle pozostałych bibliotek omówionych w ramach niniejszej pracy jest fakt, że jest ona dedykowana językowi C++, w związku z czym dużo łatwiej dostępne są wątki społecznościowe i artykuły omawiające realizację różnorodnych typów modeli z jej użyciem, oraz oferując przykładowy kod źródłowy. Strona główna projektu \cite{shark:home} posiada rozbudowaną, przejrzystą i opatrzoną przykładami dokumentację, znacznie ułatwiając korzystanie z dostępnego API.