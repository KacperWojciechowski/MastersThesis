\chapter{Shogun}

\section{Introductions}

Shogun is an open-source free machine learning library made in C++, accessible based on \textit{BSD 3-clause} license \cite{shogun:github}. It provides interfaces for various languages, such as Python, Ruby or C\#, but also allows users to use it in its native language. The library focuses on classification and regression problems.

\section{Data formats}

Base class allowing for loading data into Shogun is \textit{std::vector} present in the Standard Template Library (STL) of C++ language. As such, a user can use any mechanism of loading and parsing data, eg. from a file, network or another device, that at the end returns a vector containing the observations, as long as he provides it himself. One popular choice for storing training data is CSV file, for which Shogun provides support \cite{handsOnMachineLearning}. Still in that case data is a subject to several requirements:

\begin{itemize}
	\item \textbf{The file can only contain numeric data} - if any text data are present, preprocessing is needed to recode them into some numeric values (eg. one-hot encoding or regular subsequent numbers in terms of response variable classes). Sadly this means that data explanations cannot be stored alongside it.
	
	\item \textbf{Comma separator} - this becomes an issue if data is previously processed via other software, like Microsoft Excel or JMP, as it commonly uses semicolon as a separator. Despite the CSV format allows for several different separators, Shogun accepts only comma as a valid one.

	\item \textbf{Real values should use dot character as a decimal point} - this comes from the characteristics of C++ (and also other languages), and is required for the language to be able to automatically parse a value from text representation to numeric representation.
\end{itemize}

In order to read and parse data from CSV file, user needs to use \textit{shogun::CCSVFile} class, which result is then loaded into an object of \textit{shogun::SGMatrix}. Because of the fact that this class saved the stored data column-wise, to use them in the training process, user needs to transpose it, and then separate this matrix into two parts, one of which contains predictors, and the other labels. Sample code snippet of this procedure is shown on listing \ref{shogun:csv}. After correct data separation, the container needs to be transposed again so it will be accepted by the learning algorithm, and it needs to be wrapped into specifically designed classes called \textit{CDenseFeatures}, \textit{CMulticlassLabels} or \textit{CRegressionLabels}. This part was shown on listing \ref{shogun:csv2}.

\cppcode{Result/inc/shogun/csv.hpp}{Sample read and preprocessing of data from a CSV file}{shogun:csv}

\cppcode{Result/inc/shogun/shogunModels.hpp}{Repackaging data into desired containers}{shogun:csv2}

\section{Data processing and exploration methods}

\subsection{Normalizing}

The library provides a normalizing min-max mechanism that guarantees the data will be rescaled into a $\langle$ 0; 1 $\rangle$ range, with a class \textit{shogun::CRescaleFeatures}. An object of this class can be then used again for the new data corresponding to the previously learned predictors. Its two main methods are:

\begin{itemize}
	\item \textit{fit()} - teaches the normalizer object statistical characteristics of the provided data;
	\item \textit{transform()} - performs normalizing.
\end{itemize}

In case of some algorithms provided by Shogun, normalizing is one of the first steps executed automatically, so it is not always necessary to do this in preprocessing. Such information should be included in the documentation of a given method. Listing \ref{shogun:normalizer} shows how to use the aforementioned class. The class, aswell as the function shown on the listing perform the normalizing in-place, hence no need to override the object storing the data.

\cppcode{Result/inc/shogun/rescale.hpp}{Przykład funkcji wykonującej normalizację.}{shogun:normalizer}

\subsection{Dimensionality reduction}

Shogun gives to the user several algorithms of dimensionality reduction as following classes \cite{handsOnMachineLearning}:

\begin{itemize}
	\item \textbf{Principle Component Analysis} - class \textit{CPCA};
	\item \textbf{Kernel Principle Component Analysis} - class \textit{CKernelPCA};
	\item \textbf{Multidimensional Scaling} - class \textit{MultidimensionalScaling};
	\item \textbf{IsoMap} - class \textit{CIsoMap};
	\item \textbf{ICA} - class \textit{CFastICA};
	\item \textbf{Factor Analysis} - class \textit{CFactorAnalysis};
	\item \textbf{t-SNE} - class \textit{CTDistributedStochasticNeighborEmbedding}.
\end{itemize}

Each of the classes mentioned above operates by first learning the parameters of the training data by calling the \textit{fit()} function, and establishing the desired dimensions count (except for ICA). Trained reductor object can be used to reduce the dimensionality by \textit{apply\char`_to\char`_feature\char`_vector()} method, that returns transformed data, or in case of ICA, Factor Analysis and t-SNE, by \textit{transform()} method, which result needs to be casted into a pointer to \textit{CDenseFeatures}. Unfortunately, using any of the reductor objects requires creating a new copy of the data object during transformation, instead of performing it in-place. Listing \ref{shogun:reduction} shows an example based on class \textit{CKernelPCA}.

\cppcode{Result/inc/shogun/pca_reduction.hpp}{Dimensionality reduction using Kernel PCA \cite{handsOnMachineLearning}.}{shogun:reduction}

\subsection{L1 and L2 Regularisation}

In Shogun, regularisation is an integral part of a machine learning model, and as such, it is always performed by given model. Each model defines itself which kind of regularization it performs, and this cannot be changed.

\section{Machine Learning Models}
\subsection{Linear Regression}
One of the fundamental machine learning algorithms provided by the Shogun library is linear regression, performed by the \textit{CLinearRidgeRegression} class. As the name suggests, it utilizes Ridge Regression, configured at the model creation stage. Listing \ref{shogun:linear} shows how to adjust the model.

\cppcode{Result/inc/shogun/linear.hpp}{Linear regression example in Shogun}{shogun:linear}

\subsection{Logistic Regression}
Shogun library contains implementation of multiclass logistic regression via \textit{CMulticlassLogisticRegression} class. It also provides configurable regularisation. Listing \ref{shogun:logistic} shows how to use it.

\cppcode{Result/inc/shogun/logistic.hpp}{Logistic regression example in Shogun}{shogun:logistic}

\subsection{Support Vector Machine}
Similarly to logistic regression, Shogun provides the multiclass Support Vector Machine model implementation for classification tasks, as \textit{CMulticlassLibSVM} class. It contains several configurable parameters, along with the choice of kernel itself. Listing \ref{shogun:svm} shows an example of use.

\cppcode{Result/inc/shogun/svm.hpp}{Support Vector Machine example in Shogun}{shogun:svm}

\subsection{K-Nearest Neighbours}

K-Nearest Neighbours algorith is available as \textit{CKNN} class. It allows user to select the method of calculating distances by passing a correct object, and the count of neighbours considered the nearest. The main available distance metrics are: Euklidean, Hamming, Manhattan and cosinus similarity. In comparison to other methods, it does not require the configuration of hyperparameters, allowing the use of it in crossvalidation without problems. Listing \ref{shogun:knn} presents an example configuration and use of KNN algorithm using Euclidean distance.

\cppcode{Result/inc/shogun/knn.hpp}{KNN algorithm example in Shogun}{shogun:knn}

\subsection{Ensemble algorithms}
\subsubsection{Gradient boosting}

Implementation of ensemble algorithm using gradient boosting is adjusted only for regression models. The class \textit{CStochasticGBMachine} is responsible for its execution. It allows the user to configure several parameters, some of which are:

\begin{itemize}
	\item base algorithm;
	\item loss function;
	\item iteration count;
	\item learning coefficient;
	\item fraction of vectors to choose at each iteration.
\end{itemize}

Listing \ref{shogun:gb} shows a method of creating such model using binary decisive tree for regression and classification (implemented by \textit{CCARTree} class) as a base algorithm.

\cppcode{Result/inc/shogun/gb.hpp}{Gradient boosting example in Shogun}{shogun:gb}

\subsubsection{Random forest}
The random forest method is available in the Shogun library via the \textit{CRandomForest} class. On the contrary to gradient boosting, its implementation allows also for classification tasks. Some of the main configurable parameters are:

\begin{itemize}
	\item number of trees;
	\item number of batches to which the data should be separated;
	\item algorithm of the result selection;
	\item type of the problem;
	\item continuity of the regressors.
\end{itemize}

Listing \ref{shogun:rf} shows how to create and configure random forest model for cosinus approximation.

\cppcode{Result/inc/shogun/rforest.hpp}{Przykład użycia metody losowego lasu}{shogun:rf}

\subsection{Neural network}

First step in creating a neural network with this library is to configure the network topology, using \textit{CNeuralLayers} object. It consists of a range of methods, which create layers with given activation function:

\begin{itemize}
	\item \textit{input()} - input layer with a certain number of inputs;
	\item \textit{logistic()} - fully connected sigmoid function layer;
	\item \textit{linear()} - fully connected linear function layer;
	\item \textit{rectified\char`_linear()} - fully connected ReLU layer;
	\item \textit{leaky\char`_rectified\char`_linear} - fully connected Leaky ReLU layer;
	\item \textit{softmax} - fully connected softmax layer;
\end{itemize}

The order in which we call the methods is important, because it decides about the order of layers within the model. After completing the configuration process, it is possible to confirm the architecture by creating another object via the \textit{done()} method, and using it to initialize \textit{CNeuralNetwork} class. In order to connect the layers, the \textit{quick\char`_connect} method needs to be called, and subsequently, the weights have to be initialized via \textit{initialize\char`_neural\char`_network()} function. It accepts a parameter describing a Gaussian distribution used to initialize weights.

The next step is to configure the optimizer using \textit{set\char`_optimization()} method. The class \textit{CNeuralNetwork} supports optimizing using the steepest descent method and Broyden-Fletcher-Goldfarb-Shannon method. This model has in-built L2 regularization, which can be further configured, similarly to other parameters such as learning coefficient, epoch count, convergence criterion for the loss function, or the size of batches. Unfortunately, user cannot select the loss function in question, since it is established automatically based on the label type. Listing \ref{shogun:nn} shows the full process of building, configuring and teaching a network. Unfortunately, due to the lack of hyperbolic tangential function implementation, a reLU function was used in its stead, which affects the achieved results.

\cppcode{Result/inc/shogun/neural.hpp}{Example of neural network use in Shogun.}{shogun:nn}

\section{Model analysis methods}

\subsection{Mean squared error}

Calculation of mean squared error in the Shogun library comes down to creating an object of type \textit{CMeanSquaredError} as an argument to the \textit{some<>()} template function. It is returned as a pointer to the object. In order to acquire the value of the error for given data, user needs to call the \textit{evaluate()} method, passing prediction results and observed output. Listing \ref{shogun:mse} presents the detailed way to use it.


\cppcode{Rozdzial4/shogun-mse.cpp}{Example of calculating mean squared error in Shogun\cite{handsOnMachineLearning}}{shogun:mse}

\subsection{Mean absolute error}

Realizacja obliczania średniego błędu bezwzględnego dla biblioteki Shogun dokonywana jest za pomocą klasy \textit{CMeanAbsoluteError} pełniącej rolę ewaluatora. Tworzona jest ona poprzez wykorzystanie szablonu \textit{some<>} a następnie wykorzystywana do obliczeń wołając jej metodę \textit{evaluate} przekazując uzyskane oraz oczekiwane wyniki regresji lub klasyfikacji. Listing \ref{shogun:mae} przedstawia sposób użycia wyżej wymienionej klasy.

\cppcode{Rozdzial4/shogun-mae.cpp}{Przykład obliczenia wartości średniego błędu bezwzględnego \cite{handsOnMachineLearning}.}{shogun:mae}

\subsection{Logarytmiczna funkcja straty}

Logarytmiczna funkcja straty jest możliwa do obliczenia z wykorzystaniem biblioteki Shogun przy użyciu klasy \textit{CLogLoss}, jednak udostępniane przez nią metody wskazują że powinna być wykorzystywana przez model, a nie bezpośrednio przez użytkownika. Udostępnia ona metodę \textit{get\char`_square\char`_grad()} pozwalającą na obliczenie kwadratu gradientu między zadaną predykcją a docelowym wynikiem. Sposób użycia tej metody zaprezentowano na listingu \ref{shogun:log}

\cppcode{Rozdzial4/shogun-log.cpp}{Przykład użycia klasy \textit{CLogLoss}.}{shogun:log}

\subsection{Metryka $R^2$}

Biblioteka Shogun nie posiada bezpośredniej implementacji dla metryki $R^2$ w związku z czym, pomimo możliwości wykorzystania wbudowanej metody obliczania błędu średniokwadratowego, wariancja odpowiedzi potrzebna dla uzyskania wyniku musi zostać uzyskana przez własny mechanizm użytkownika. Listing \ref{shogun:verify} przedstawia funkcję weryfikującą poprawność modeli opisanych w poprzednich punktach, obliczającą wartość metryki $R^2$.

\cppcode{Result/inc/shogun/verify.hpp}{Przykład obliczenia metryki $R^2$.}{shogun:verify}

\subsection{Dokładność}

Do obliczenia dokładności w przypadku zadań regresji, biblioteka udostępnia klasę \textit{CMulticlassAccuracy}. Pozwala ona nie tylko na samą klasyfikację, lecz także oferuje metodę pobrania macierzy błędnych klasyfikacji. Nie znaleziono natomiast klasy \textit{CAccuracyMeasure} wspomnianej w pracy \cite{handsOnMachineLearning}, co sugerowałoby jej usunięcie z biblioteki. Listing \ref{shogun:acc} pokazuje w jaki sposób należy użyć klasy \textit{CMulticlassAccuracy}.

\cppcode{Rozdzial4/shogun-acc.cpp}{Przykład obliczenia dokładności modelu.}{shogun:acc} 

\subsection{Precyzja i pełność (recall), oraz metryka F1}

W podręczniku \cite{handsOnMachineLearning} wspomniane zostały klasy \textit{CRecallMeasure} oraz \textit{CF1Measure} mające pozwolić obliczyć odpowiednio pamięć modelu oraz metrykę F1, jednak w trakcie pracy z biblioteką nie znaleziono definicji tych klas, lub jakichkolwiek innych pełniących te funkcje, w związku z czym założono brak implementacji tych metod dla biblioteki Shogun. Zdecydowano się na wspomnienie o tym fakcie, w celu podkreślenia możliwoścy wystąpienia rozbieżności między źródłami wiedzy, a aktualnym stanem biblioteki.

\subsection{Pole pod wykresem krzywej operacyjnej}

Biblioteka Shogun posiada implementację obliczania pola pod wykresem krzywej charakterystycznej odbiornika, w postaci klasy \textit{CROCEvaluation}. Listing \ref{shogun:roc} przedstawia sposób jej użycia.

\cppcode{Rozdzial4/shogun-roc.cpp}{Przykład obliczenia pola pod wykresem funkcji ROC dla Shogun.}{shogun:roc}

\subsection{K-krotny sprawdzian krzyżowy}

Sprawdzian krzyżowy stanowi w bibliotece Shogun złożony mechanizm, do którego wykorzystania należy przygotować drzewo decyzyjne parametrów, reprezentowane przez klasę \textit{CModelSelectionParameters}. Użytkownik może wybrać model oraz kryterium ewaluacji modelu poprzez utworzenie odpowiednich klas, a następnie przekazanie ich w konstruktorze obiektu sprawdzianu krzyżowego, będącego instancją klasy \textit{CCrossValidation}. Kolejnym krokiem jest utworzenie instancji klasy \textit{CGridSearchModelSelection} która dokona wyboru parametrów. Ostatnim etapem jest konfiguracja docelowego modelu i przeprowadzenie procesu uczenia. Dokłady wygląd całego mechanizmu został przedstawiony na listingu \ref{shogun:cross}.

\cppcode{Result/inc/shogun/cross.hpp}{Przygotowanie modelu wieloklasowej regresji liniowej z wykorzystaniem sprawdzianu krzyżowego.}{shogun:cross}

\section{Dostępność dokumentacji i źródeł wiedzy}

Internetowe źródła informacji w postaci forów społecznościowych skupiają się na wykorzystaniu biblioteki Shark w innych językach, jak np. Python, lecz wraz z jej kodem źródłowym na platformie GitHub \cite{shogun:github} możliwe jest wygenerowanie przykładów jej wykorzystania także w języku C++ w folderze \textit{examples}. Przykłady te należy zbudować za pomocą odpowiednego skryptu Pythona zawartego w repozytorium, powodując wygenerowanie listingów kodów w docelowym języku w plikach JSON. Niestety, okazują się one obrazować użycie biblioteki w nienaturalny, proceduralnie generowany sposób, sprawiając, że przy faktycznej próbie skorzystania z API projektu, stają się one bezużyteczne. Dodatkowo, jedyna forma dokumentacji projektu ogranicza się do komentarzy w kodzie, zmuszając użytkownika do błądzenia po repozytorum w poszukiwaniu potrzebnych informacji. 

Shogun jest jedną z bibliotek opisaną w podręczniku \cite{handsOnMachineLearning}, wprowadzającym czytelnika zarówno do podstawowych funkcjonalności Shogun, jak i podsumowującej podstawy teorii uczenia maszynowego w kontekście ich zastosowania. Większość z przykładów realizacji poszczególnych typów modeli w tej książce posiada przedstawione główne fragmenty listingów dla biblioteki Shogun. Warto jednak zaznaczyć, że różnią się one od przykładów generowanych przez skrypt budujący, obrazując wykorzystanie faktycznie udostępnianego API biblioteki. W toku pracy nad niniejszym rozdziałem, książka ta okazała się jedynym wartościowym źródłem wiedzy na jego temat.