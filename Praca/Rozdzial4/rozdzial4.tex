\chapter{Shogun}

\section{Introductions}

Shogun is an open-source free machine learning library made in C++, accessible based on \textit{BSD 3-clause} license \cite{shogun:github}. It provides interfaces for various languages, such as Python, Ruby or C\#, but also allows users to use it in its native language. The library focuses on classification and regression problems.

\section{Data formats}

Base class allowing for loading data into Shogun is \textit{std::vector} present in the Standard Template Library (STL) of C++ language. As such, a user can use any mechanism of loading and parsing data, eg. from a file, network or another device, that at the end returns a vector containing the observations, as long as he provides it himself. One popular choice for storing training data is CSV file, for which Shogun provides support \cite{handsOnMachineLearning}. Still in that case data is a subject to several requirements:

\begin{itemize}
	\item \textbf{The file can only contain numeric data} - if any text data are present, preprocessing is needed to recode them into some numeric values (eg. one-hot encoding or regular subsequent numbers in terms of response variable classes). Sadly this means that data explanations cannot be stored alongside it.
	
	\item \textbf{Comma separator} - this becomes an issue if data is previously processed via other software, like Microsoft Excel or JMP, as it commonly uses semicolon as a separator. Despite the CSV format allows for several different separators, Shogun accepts only comma as a valid one.

	\item \textbf{Real values should use dot character as a decimal point} - this comes from the characteristics of C++ (and also other languages), and is required for the language to be able to automatically parse a value from text representation to numeric representation.
\end{itemize}

In order to read and parse data from CSV file, user needs to use \textit{shogun::CCSVFile} class, which result is then loaded into an object of \textit{shogun::SGMatrix}. Because of the fact that this class saved the stored data column-wise, to use them in the training process, user needs to transpose it, and then separate this matrix into two parts, one of which contains predictors, and the other labels. Sample code snippet of this procedure is shown on listing \ref{shogun:csv}. After correct data separation, the container needs to be transposed again so it will be accepted by the learning algorithm, and it needs to be wrapped into specifically designed classes called \textit{CDenseFeatures}, \textit{CMulticlassLabels} or \textit{CRegressionLabels}. This part was shown on listing \ref{shogun:csv2}.

\cppcode{Result/inc/shogun/csv.hpp}{Sample read and preprocessing of data from a CSV file}{shogun:csv}

\cppcode{Result/inc/shogun/shogunModels.hpp}{Repackaging data into desired containers}{shogun:csv2}

\section{Data processing and exploration methods}

\subsection{Normalizing}

The library provides a normalizing min-max mechanism that guarantees the data will be rescaled into a $\langle$ 0; 1 $\rangle$ range, with a class \textit{shogun::CRescaleFeatures}. An object of this class can be then used again for the new data corresponding to the previously learned predictors. Its two main methods are:

\begin{itemize}
	\item \textit{fit()} - teaches the normalizer object statistical characteristics of the provided data;
	\item \textit{transform()} - performs normalizing.
\end{itemize}

In case of some algorithms provided by Shogun, normalizing is one of the first steps executed automatically, so it is not always necessary to do this in preprocessing. Such information should be included in the documentation of a given method. Listing \ref{shogun:normalizer} shows how to use the aforementioned class. The class, aswell as the function shown on the listing perform the normalizing in-place, hence no need to override the object storing the data.

\cppcode{Result/inc/shogun/rescale.hpp}{Przykład funkcji wykonującej normalizację.}{shogun:normalizer}

\subsection{Dimensionality reduction}

Shogun gives to the user several algorithms of dimensionality reduction as following classes \cite{handsOnMachineLearning}:

\begin{itemize}
	\item \textbf{Principle Component Analysis} - class \textit{CPCA};
	\item \textbf{Kernel Principle Component Analysis} - class \textit{CKernelPCA};
	\item \textbf{Multidimensional Scaling} - class \textit{MultidimensionalScaling};
	\item \textbf{IsoMap} - class \textit{CIsoMap};
	\item \textbf{ICA} - class \textit{CFastICA};
	\item \textbf{Factor Analysis} - class \textit{CFactorAnalysis};
	\item \textbf{t-SNE} - class \textit{CTDistributedStochasticNeighborEmbedding}.
\end{itemize}

Each of the classes mentioned above operates by first learning the parameters of the training data by calling the \textit{fit()} function, and establishing the desired dimensions count (except for ICA). Trained reductor object can be used to reduce the dimensionality by \textit{apply\char`_to\char`_feature\char`_vector()} method, that returns transformed data, or in case of ICA, Factor Analysis and t-SNE, by \textit{transform()} method, which result needs to be casted into a pointer to \textit{CDenseFeatures}. Unfortunately, using any of the reductor objects requires creating a new copy of the data object during transformation, instead of performing it in-place. Listing \ref{shogun:reduction} shows an example based on class \textit{CKernelPCA}.

\cppcode{Result/inc/shogun/pca_reduction.hpp}{Dimensionality reduction using Kernel PCA \cite{handsOnMachineLearning}.}{shogun:reduction}

\subsection{L1 and L2 Regularisation}

In Shogun, regularisation is an integral part of a machine learning model, and as such, it is always performed by given model. Each model defines itself which kind of regularization it performs, and this cannot be changed.

\section{Machine Learning Models}
\subsection{Linear Regression}
One of the fundamental machine learning algorithms provided by the Shogun library is linear regression, performed by the \textit{CLinearRidgeRegression} class. As the name suggests, it utilizes Ridge Regression, configured at the model creation stage. Listing \ref{shogun:linear} shows how to adjust the model.

\newpage
\cppcode{Result/inc/shogun/linear.hpp}{Linear regression example in Shogun}{shogun:linear}

\subsection{Logistic Regression}
Shogun library contains implementation of multiclass logistic regression via \textit{CMulticlassLogisticRegression} class. It also provides configurable regularisation. Listing \ref{shogun:logistic} shows how to use it.

\cppcode{Result/inc/shogun/logistic.hpp}{Logistic regression example in Shogun}{shogun:logistic}

\subsection{Support Vector Machine}
Similarly to logistic regression, Shogun provides the multiclass Support Vector Machine model implementation for classification tasks, as \textit{CMulticlassLibSVM} class. It contains several configurable parameters, along with the choice of kernel itself. Listing \ref{shogun:svm} shows an example of use.


\hspace{20px}
\cppcode{Result/inc/shogun/svm.hpp}{Support Vector Machine example in Shogun}{shogun:svm}

\subsection{K-Nearest Neighbours}

K-Nearest Neighbours algorith is available as \textit{CKNN} class. It allows user to select the method of calculating distances by passing a correct object, and the count of neighbours considered the nearest. The main available distance metrics are: Euklidean, Hamming, Manhattan and cosinus similarity. In comparison to other methods, it does not require the configuration of hyperparameters, allowing the use of it in crossvalidation without problems. Listing \ref{shogun:knn} presents an example configuration and use of KNN algorithm using Euclidean distance.

\cppcode{Result/inc/shogun/knn.hpp}{KNN algorithm example in Shogun}{shogun:knn}

\subsection{Ensemble algorithms}
\subsubsection{Gradient boosting}

Implementation of ensemble algorithm using gradient boosting is adjusted only for regression models. The class \textit{CStochasticGBMachine} is responsible for its execution. It allows the user to configure several parameters, some of which are:

\begin{itemize}
	\item base algorithm;
	\item loss function;
	\item iteration count;
	\item learning coefficient;
	\item fraction of vectors to choose at each iteration.
\end{itemize}

Listing \ref{shogun:gb} shows a method of creating such model using binary decisive tree for regression and classification (implemented by \textit{CCARTree} class) as a base algorithm.

\cppcode{Result/inc/shogun/gb.hpp}{Gradient boosting example in Shogun}{shogun:gb}

\subsubsection{Random forest}
The random forest method is available in the Shogun library via the \textit{CRandomForest} class. On the contrary to gradient boosting, its implementation allows also for classification tasks. Some of the main configurable parameters are:

\begin{itemize}
	\item number of trees;
	\item number of batches to which the data should be separated;
	\item algorithm of the result selection;
	\item type of the problem;
	\item continuity of the regressors.
\end{itemize}

Listing \ref{shogun:rf} shows how to create and configure random forest model for cosinus approximation.

\cppcode{Result/inc/shogun/rforest.hpp}{Przykład użycia metody losowego lasu}{shogun:rf}

\subsection{Neural network}

First step in creating a neural network with this library is to configure the network topology, using \textit{CNeuralLayers} object. It consists of a range of methods, which create layers with given activation function:

\begin{itemize}
	\item \textit{input()} - input layer with a certain number of inputs;
	\item \textit{logistic()} - fully connected sigmoid function layer;
	\item \textit{linear()} - fully connected linear function layer;
	\item \textit{rectified\char`_linear()} - fully connected ReLU layer;
	\item \textit{leaky\char`_rectified\char`_linear} - fully connected Leaky ReLU layer;
	\item \textit{softmax} - fully connected softmax layer;
\end{itemize}

The order in which we call the methods is important, because it decides about the order of layers within the model. After completing the configuration process, it is possible to confirm the architecture by creating another object via the \textit{done()} method, and using it to initialize \textit{CNeuralNetwork} class. In order to connect the layers, the \textit{quick\char`_connect} method needs to be called, and subsequently, the weights have to be initialized via \textit{initialize\char`_neural\char`_network()} function. It accepts a parameter describing a Gaussian distribution used to initialize weights.

The next step is to configure the optimizer using \textit{set\char`_optimization()} method. The class \textit{CNeuralNetwork} supports optimizing using the steepest descent method and Broyden-Fletcher-Goldfarb-Shannon method. This model has in-built L2 regularization, which can be further configured, similarly to other parameters such as learning coefficient, epoch count, convergence criterion for the loss function, or the size of batches. Unfortunately, user cannot select the loss function in question, since it is established automatically based on the label type. Listing \ref{shogun:nn} shows the full process of building, configuring and teaching a network. Unfortunately, due to the lack of hyperbolic tangential function implementation, a reLU function was used in its stead, which affects the achieved results.

\cppcode{Result/inc/shogun/neural.hpp}{Example of neural network use in Shogun.}{shogun:nn}

\section{Model analysis methods}

\subsection{Mean squared error}

Calculation of mean squared error in the Shogun library comes down to creating an object of type \textit{CMeanSquaredError} as an argument to the \textit{some<>()} template function. It is returned as a pointer to the object. In order to acquire the value of the error for given data, user needs to call the \textit{evaluate()} method, passing prediction results and observed output. Listing \ref{shogun:mse} presents the detailed way to use it.


\cppcode{Rozdzial4/shogun-mse.cpp}{Example of calculating mean squared error in Shogun\cite{handsOnMachineLearning}}{shogun:mse}

\subsection{Mean absolute error}

Mean absolute error calculation is carried out by the \textit{CMeanAbsoluteError} class, which serves as an evaluator. It is created by a call to \textit{some<>()} function and used for evaluation by calling the \textit{evaluate()} method, providing the results of a model and expected labels of regression or classification. Listing \ref{shogun:mae} shows how to use the aforementioned class.

\cppcode{Rozdzial4/shogun-mae.cpp}{Example of mean absolute error calculation using Shogun\cite{handsOnMachineLearning}.}{shogun:mae}

\subsection{Logarithmic loss function}

Logarithmic loss function is possible to calculate using the Shogun library using \textit{CLogLoss} class, however the public interface of said class indicate that it should be rather used by a model than directly by the user. It provides \textit{get\char`_square\char`_grad()} method which allows for calculation of the squared gradient between prediction and expected result. The use case is presented on listing \ref{shogun:log}.

\cppcode{Rozdzial4/shogun-log.cpp}{Example of use for \textit{CLogLoss} class.}{shogun:log}

\subsection{$R^2$ metric}

The Shogun library does not provide implementation for the $R^2$ metric as such, so despite the ability to use the in-built method of mean squared error calculation, the variance of output needed for calculating the metric needs to be acquired from a user-provided mechanism. Listing \ref{shogun:verify} shows a function that verifies the correctness of models described in previous paragraphs by calculating the $R^2$ metric.

\cppcode{Result/inc/shogun/verify.hpp}{Example of $R^2$ metric calculation.}{shogun:verify}

\subsection{Accuracy}

In order to calculate the accuracy metric for regression tasks, this library provides \textit{CMulticlassAccuracy} class. It also allows for getting a misclassification matrix. The \textit{CAccuracyMeasure} class mentioned in \cite{handsOnMachineLearning} however was not found, which indicates it was removed from the library. Listing \ref{shogun:acc} shows how to use \textit{CMulticlassAccuracy}.

\cppcode{Rozdzial4/shogun-acc.cpp}{Example of accuracy calculation in Shogun.}{shogun:acc} 

\subsection{Precision, Recall and F1 metric}

The \cite{handsOnMachineLearning} handbook mentions \textit{CRecallMeasure} and \textit{CF1Measure} classes that are supposed to allow the user to calculate the recall and F1 metric of the model, however while working with the library, their definitions were not found, neither any other classes of similar functionality, and as such it was assumed that the Shogun library does not provide the implementation for them. The motivation to touch on this topic was to inform the reader of possible discrepancies between available sources and current state of the library. 

\subsection{ROC curve}

The Shogun library provides implementation to calculate ROC curve as the \textit{CROCEvaluation} class. Listing \ref{shogun:roc} shows the way how to use it.

\cppcode{Rozdzial4/shogun-roc.cpp}{ROC curve calculation example for Shogun.}{shogun:roc}

\subsection{K-fold cross-validation}

Cross-validation inside the Shogun library is a complex mechanism, which in order to use it requires a decision tree of available parameters, represented by the \textit{CModelSelectionParameters} class. The user can select the model and evaluation criterion for it by creating objects of selected classes and providing them to the \textit{CCrossValidation} instance constructor. The next step is to create an instance of \textit{CGridSearchModelSelection} which will select parameters. The last phase is to configure the desired model and complete the teaching process. Precise example of the whole mechanism was presented on listing \ref{shogun:cross}.

\cppcode{Result/inc/shogun/cross.hpp}{Preparation of multiclass classification via linear regression with cross-validation in Shogun.}{shogun:cross}

\section{Documentation and sources availability}

Internet sources such as community forums focus on using Shogun with different programming languages such as eg. Python, however using its source code available on GitHub \cite{shogun:github} it is possible to generate examples of its use in C++ language aswell in the \textit{examples} directory. Those examples need to be build using a specific Python script contained within the repository, that generates listings of codes in desired language in JSON files. Unfortunately, they present the use of the library in very unnatural, procedurally generated manner, causing them to be useless when attempting to use the actual project's API. In addition, the only form of project documentation is restricted to the comments within the source code, making the user wander through the repository in search of the needed information.

The Shogun is one of the libraries described within the \textit{"Hands-On Machine Learning with C++"}\cite{handsOnMachineLearning} handbook, which both introduces the reader to basic functionalities of the library, aswell as summarizes the basics of machine learning theory in order to use it. Majority of various model types examples within this book is equipped with listings of main code snippets for the Shogun library. It is important to mention however, that they are different from the examples generated by the build script, showing the use of actual API of the library. During the making of this chapter, this handbook proved to be the only valuable source on the topic.