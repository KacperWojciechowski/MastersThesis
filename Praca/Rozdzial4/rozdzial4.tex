\chapter{Biblioteka Shogun}

\section{Wprowadzenie}

Shogun to darmowa biblioteka do uczenia maszynowego o otwartym źródle, napisana w C++ i udostępniana według licencji \textit{BSD 3-clause} \cite{shogun:github}. Posiada ona interfejsy dla różnych języków, w tym Python, Ruby czy C\#, jednak pozwala ona na jej użycie także w jej natywnym języku. Skupia się ona na problemach klasyfikacji oraz regresji. 

\section{Formaty źródeł danych}

Podstawową klasą pozwalającą na załadowanie danych do biblioteki Shogun jest klasa \textit{std::vector} z standardowej biblioteki szablonowej (ang. \textit{Standard Template Library, STL}) języka C++. W związku z tym, do pobrania danych dla programu realizującego nauczanie i pracę z modelem możliwe jest wykorzystanie dowolnego mechanizmu (np. odczytu z pliku, pobranie danych z sieci czy innego urządzenia) które finalnie przetworzy je do postaci wektora, lecz należy ten mechanizm dostarczyć we własnym zakresie. Popularnym wyborem do przechowywania informacji uczących jest plik o ustrukturyzowanym formacie CSV, dla którego biblioteka Shogun posiada dedykowane wsparcie \cite{handsOnMachineLearning}. Obwarowane jest ono jednak pewnymi wymaganiami:

\begin{itemize}
	\item \textbf{Plik musi zawierać jedynie dane numeryczne} - wprzypadku występowania wartości tekstowych, należy wykonać przetwarzanie wstępne mające na celu ich zamianę na wartości liczbowe (np. w przypadku klas decyzyjnych zmiennej odpowiedzi sugerowane jest zastosowanie kodowania \textit{one-hot}). Niestety ten wymóg nie pozwala na przechowywanie etykiet wraz z danymi.
	
	\item \textbf{Jako separator należy użyć przecinka} - mimo iż sam format, jak i wiele programów komercyjnych do pracy z danymi, jak np. Microsoft Excel, JMP, itp., pozwalają na zastosowanie innych separatorów, takich jak średnik, dla biblioteki Shogun należy zastosować w formie separatora przecinek;
	
	\item \textbf{Liczby rzeczywiste powinny być zapisywane z użyciem kropki jako separatora dziesiętnego} - wynika to ze specyfiki języka C++ (jak i wielu innych języków), że domyślne mechanizmy wymuszają użycie kropki jako separatora dziesiętnego, i oczekują jej w przypadku parsowania liczby rzeczywistej z postaci ciągu znakowego odczytanego z pliku, do postaci wartości liczbowej.
\end{itemize} 

Do odczytu i parsowania danych z pliku CSV wykorzystywana jest klasa \textit{shogun::CCSVFile}, której wynik następnie ładowany jest do klasy \textit{shogun::SGMatrix}. Ze względu na zapis odczytanych danych w kolejności według kolumn. do wykorzystania ich w procesie uczenia konieczna jest transpozycja, a następnie rozdzielenie macierzy na dwie części, z których jedna zawiera regresory, a druga wartości zmiennej odpowiedzi. Przykładowy fragment kodu realizujący to zadanie zamieszczony został na listingu \ref{shogun:csv}.

\cppcode{Rozdzial4/shogun-csv.cpp}{Przykładowy program do odczytu i przygotowania danych z pliku CSV dla biblioteki Shogun \cite{handsOnMachineLearning}}{shogun:csv}

\section{Metody przetwarzania i eksploracji danych}

\subsection{Normalizacja}

     
Biblioteka dostarcza możliwość normalizacji typu min-max, zapewniając że dane mieścić się będą w przedziale jednostkowym, za pomocą klasy \textit{shogun::CRescaleFeatu-res}. W przypadku niektórych algorytmów oferowanych przez Shogun, normalizacja jest jednym z pierwszych wykonywanych kroków, w związku z czym nie zawsze jest potrzeba wykonania jej we wstępnym przetwarzaniu. Informacja o takim przypadku powinna być zawarta w dokumentacji danej metody. Klasa pozwala na ponowne wykorzystanie dla danych o tych samych nauczonych zmiennych statystycznych. Klasa \textit{shogun::CReslaceFeatures} posiada dwie główne metody:

\begin{itemize}
	\item \textit{fit()} - pozwalającą na nauczenie normalizatora statystyk danych;
	\item \textit{transform()} - pozwalającą na normalizację obserwacji.
\end{itemize}

Listing \ref{shogun:normalizer} pokazuje jak wykorzystać wyżej wspomnianą klasę do zrealizowania normalizacji.

\cppcode{Rozdzial4/shogun-normalizer.cpp}{Przykład wykonania normalizacji \cite{handsOnMachineLearning}}{shogun:normalizer}

\subsection{Redukcja wymiarowości}

Shark udostępnia użytkownikowi kilka rodzajów algorytmów redukcji wymiarowości, realizowane przez następujące klasy \cite{handsOnMachineLearning}:

\begin{itemize}
	\item \textbf{PCA} - klasa \textit{CPCA};
	\item \textbf{Kernel PCA} - klasa \textit{CKernelPCA};
	\item \textbf{MDS} - klasa \textit{MultidimensionalScaling};
	\item \textbf{IsoMap} - klasa \textit{CIsoMap};
	\item \textbf{ICA} - klasa \textit{CFastICA};
	\item \textbf{Factor analysis} - klasa \textit{CFactorAnalysis};
	\item \textbf{t-SNE} - klasa \textit{CTDistributedStochasticNeighborEmbedding}.
\end{itemize}

Każda z powyższych klas operuje poprzez poprzednie nauczenie się parametrów danych uczących metodą \textit{fit()} oraz ustawienie docelowej ilości wymiarów (z wyjątkiem kernel PCA i ISA). Nauczony obiekt reduktora można wykorzystać do redukcji wymiarowości danych poprzez metodę \textit{transform()}, której wynik należy zrzutować na wskaźnik na CDenseFeatures. Listing \ref{shogun:reduction} przedstawia sposób wykonania redukcji na przykładzie klasy \textit{CKernelPCA}.

\cppcode{Rozdzial4/shogun-kernelpca.cpp}{Przykład redukcji wymiarowości z wykorzystaniem metody Kernel PCA \cite{handsOnMachineLearning}}{shogun:reduction}

\subsection{Regularyzacja L1 i L2}

W przypadku biblioteki Shogun, regularyzacja stanowi integralną część modelu, co oznacza że występuje ona zawsze podczas wykorzystania danego typu modelu uczenia maszynowego, oraz nie ma możliwości zmiany typu regularyzacji używanej przez docelowy model.

\section{Modele uczenia maszynowego}
\subsection{Regresja liniowa}
Jednym z podstawowych algorytmów uczenia maszynowego udostępnianych przez bibliotekę Shogun jest regresja liniowa, realizowana za pomocą klasy \textit{CLinearRidgeRegression}. Polega ona na dekompozycji macierzy Choleskiego \cite{handsOnMachineLearning}, wykorzystując podejście nie-iteracyjne. Jak wskazuje nazwa, metoda ta posiada wbudowaną regularyzację L2 (Ridge), której konfiguracja odbywa się podczas tworzenia obiektu modelu. Listing \ref{shogun:linear} przedstawia sposób wykonania regresji liniowej z pomocą Shogun.

\cppcode{Rozdzial4/shogun-linear.cpp}{Przykład regresji liniowej w Shogun \cite{handsOnMachineLearning}}{shogun:linear}

\subsection{Regresja logistyczna}
Biblioteka Shogun zawiera implementację wieloklasowej regresji logistycznej w postaci gotowego obiektu klasy \textit{CMulticlassLogisticRegression}. Posiada ona wbudowaną konfigurowalną regularyzację. Listing \ref{shogun:logistic} przedstawia sposób użycia wspomnianej klasy opisany w książce ,,Hands On Machine Learning'' \cite{handsOnMachineLearning}, wykorzystując dodatkowo mechanizm sprawdzianu krzyżowego do wyboru hiperparametrów, który opisany jest w dalszej części pracy.

\cppcode{Rozdzial4/shogun-logistic.cpp}{Przykład regresji logistycznej z użyciem sprawdzianu krzyżowego \cite{handsOnMachineLearning}}{shogun:logistic}

\subsection{Maszyna wektorów nośnych}
Podobnie jak w przypadku regresji logistycznej, w bibliotece Shogun dostępna jest implementacja wieloklasowej klasyfikacji z wykorzystaniem maszyny wektorów nośnych, w postaci klasy \textit{CMulticlassLibSVM}. Posiada ona szereg dostępnych do konfiguracji parametrów, i umożliwia wybór zastosowanego jądra użytkownikowi. Listing \ref{shogun:svm} prezentuje jak wykorzystać wymienioną klasę wraz z doborem parametrów \cite{handsOnMachineLearning}.

\cppcode{Rozdzial4/shogun-svm.cpp}{Przykład użycia maszyny wektorów nośnych \cite{handsOnMachineLearning}}{shogun:svm}

\subsection{Algorytm K najbliższych sąsiadów}

Algorytm K najbliższych sąsiadów dostępny jest pod postacią klasy \textit{CKNN}. Umożliwia on wybranie sposobu obliczania dystansu poprzez przekazanie obiektu odpowiedniej klasy, oraz ilości najbliższych sąsiadów. Głównymi z dostępnych typów dystansów są dystans Euklidesa, Hamminga, Manhatanu oraz podobieństwo kosinusowe. W porównaniu do poprzednich metod, nie wymaga on ustawiania hiperparametrów, dzięki czemu można z niego bezproblemowo korzystać bez sprawdzianu krzyżowego. Listing \ref{shogun:knn} pokazuje przykład konfiguracji i użycia algorytmu kNN z użyciem dystansu Euklidesa.

\cppcode{Rozdzial4/shogun-knn.cpp}{Przykład algorytmu kNN w Shogun \cite{handsOnMachineLearning}}{shogun:knn}

\subsection{Algorytm zbiorowy}
\subsubsection{Wzmacnianie gradientu}
Implementacja algorytmu zbiorowego z wykorzystaniem metody wzmacniania gradientu przystosowana jest do działania jedynie z modelami wykonującymi zadanie regresji. Klasa odpowiedzialna za jego realizację to \textit{CStochasticGBMachine}. Pozwala ona na konfigurację szeregu parametrów, do których należą:

\begin{itemize}
	\item bazowy algorytm;
	\item funkcja straty;
	\item liczba iteracji;
	\item współczynnik uczenia;
	\item ułamek wektorów do losowego wybrania w każdej iteracji.
\end{itemize}

Listing \ref{shogun:gb} przedstawia sposób implementacji powyższej metody z wykorzystaniem binarnego drzewa decyzyjnego regresji i klasyfikacji (implementowanego przez klasę \textit{CCARTree}) jako algorytm bazowy. 

\cppcode{Rozdzial4/shogun-gb.cpp}{Przykład użycia metody wzmacniania gradientu \cite{handsOnMachineLearning}}{shogun:gb}

\subsubsection{Losowy las}
Metoda losowego lasu dostępna jest w bibliotece Shogun poprzez użycie klasy \textit{CRandomForest}. W przeciwieństwie do wzmacniania gradientu, implementacja tej metody pozwala także na wykonywanie klasyfikacji. Do głównych konfigurowalnych parametrów należą:

\begin{itemize}
	\item ilość drzew;
	\item liczba zbiorów na które powinny zostać podzielone dane;
	\item algorytm wybrania końcowego wyniku;
	\item typ rozwiązywanego problemu;
	\item ciągłość wartości regresorów.
\end{itemize}

Listing \ref{shogun:rf} pokazuje jak utworzyć i skonfigurować model losowego lasu do wykonania zadania aproksymacji funkcji kosinus.

\cppcode{Rozdzial4/shogun-rf.cpp}{Przykład użycia metody losowego lasu \cite{handsOnMachineLearning}}{shogun:rf}

\subsection{Sieć neuronowa}

Pierwszym krokiem tworzenia sieci neuronowej dla niniejszej biblioteki jest skonfigurowanie architektury sieci za pomocą obiektu klasy \textit{CNeuralLayers}. Posiada ona szereg metod, które tworzą odpowiednio skonfigurowane warstwy z wybraną funkcją aktywacji:

\begin{itemize}
	\item \textit{input()} - warstwa wejściowa z określoną ilością wymiarów;
	\item \textit{logistic()} - warstwa w pełni połączona z sigmoidalną funkcją aktywacji;
	\item \textit{linear()} - warstwa w pełni połączona z liniową funkcją aktywacji;
	\item \textit{rectified\char`_linear()} - warstwa w pełni połączona z funkcją aktywacji ReLU;
	\item \textit{leaky\char`_rectified\char`_linear} - warstwa w pełni połączona z funkcją aktywacji Leaky ReLU;
	\item \textit{softmax} - warstwa w pełni połączona z funkcją aktywacji softmax. 
\end{itemize}

Kolejność wywoływania powyższych metod jest istotna, ponieważ decyduje ona o kolejności warstw w modelu. Po zakończeniu konfiguracji, możliwe jest utworzenie obiektu zatwierdzonej architektury za pomocą funkcji \textit{done()}, a następnie wykorzystanie go do inicjalizacji klasy \textit{CNeuralNetwork}. W celu połączenia warstw, należy wywołać na obiekcie sieci neuronowej funkcję \textit{quick\char`_connect} oraz zainicjalizować wagi metodą \textit{initialize\char`_neural\char`_network}. Może ona przyjąć parametr określający rozkład Gaussa używany do inicjalizacji parametrów.

Następnym krokiem jest skonfigurowanie optymalizatora za pomocą metody \textit{set\char`_optimization}. Klasa \textit{CNeuralNetwork} wspiera optymalizację z wykorzystaniem metody spadku gradientu oraz Broydena-Fletchera-Goldfarba-Shannona. Sieć neuronowa posiada wbudowaną regularyzację L2, którą można skonfigurować, podobnie jak pozostałe parametry takie jak współczynnik uczenia, ilość epok, kryterium zbieżności dla funkcji straty, czy wielkość zestawów \textit{batch}. Niestety, niemożliwy jest wybór funkcji straty, gdyż jest on dokonywany automatycznie na podstawie typu zmiennej odpowiedzi. Listing \ref{shogun:nn} przedstawia pełny proces budowania, konfiguracji oraz uczenia sieci.

\cppcode{Rozdzial4/shogun-nn.cpp}{Przykład użycia sieci neurnowej \cite{handsOnMachineLearning}}{shogun:nn}

\subsection{Brzegowa regresja jądra ?}

\section{Metody analizy modeli}

\subsection{Błąd średniokwadratowy}

Obliczenie błędu średniokwadratowego w bibliotece Shogun sprowadza się do utworzenia obiektu wykorzystującego typ \textit{CMeanSquaredError} jako argument szablonu funkcji \textit{some<>()}. Jest on zwracany pod postacią wskaźnika. W celu otrzymania wartości błędu dla posiadanych danych, należy wywołać z jego pomocą funkcję evaluate, do której przekazany zostaje zestaw predykcji oraz oczekiwanych wartości. Listing \ref{shogun:mse} ukazuje sposób użycia wspomnianego mechanizmu.

\cppcode{Rozdzial4/shogun-mse.cpp}{Przykład obliczenia wartości błędu średniokwadratowego \cite{handsOnMachineLearning}}{shogun:mse}

\subsection{Średni błąd absolutny}

\subsection{Logarytmiczna funkcja straty}
\subsection{Metryka $R^2$}
\subsection{Metryka adjusted $R^2$}
\subsection{Dokładność}
\subsection{Precyzja i pamięć (recall)}
\subsection{Metryka F-score}
\subsection{Metryki AUC i ROC}

\subsection{Sprawdzian krzyżowy K-krotny}


\section{Dostępność dokumentacji i źródeł wiedzy}

Internetowe źródła informacji w postaci forów społecznościowych skupiają się na wykorzystaniu biblioteki Shark w innych językach, jak np. Python, lecz wraz z jej kodem źródłowym na platformie GitHub \cite{shogun:github} możliwe jest znalezienie wielu przykładów jej wykorzystania także w języku C++ w folderze examples. Przykłady te należy zbudować za pomocą odpowiednego skryptu Pythona zawartego w repozytorium, powodując wygenerowanie listingów kodów w docelowym języku w plikach JSON. Ponadto, Shogun jest jedną z bibliotek opisaną w książce ,,\textit{Hands On Machine Learning with C++}'' autorstwa Kirilla Kolodiazhnyi \cite{handsOnMachineLearning}, wprowadzającej czytelnika zarówno do podstawowych funkcjonalności Shogun, jak i podsumowującej podstawy teorii uczenia maszynowego w kontekście ich zastosowania. Większość z przykładów realizacji poszczególnych typów modeli w tej książce posiada przedstawione główne fragmenty listingów dla biblioteki Shogun.

\section{Przykłady testowe}

OPISAĆ PREPROCESSING CSV !!!!

\subsection{Regresja logistyczna}
\subsection{Maszyna wektorów nośnych}
\subsection{Sieć neuronowa}